{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537f3bd1",
   "metadata": {},
   "source": [
    "# Complete Medical LLM Assistant Pipeline\n",
    "\n",
    "**Author:** Yassin Hagenimana  \n",
    "**Project:** Fine-tuning TinyLlama for Medical Question-Answering  \n",
    "**Dataset:** Medical Meadow Medical Flashcards (5,000 samples)  \n",
    "**Method:** LoRA (Low-Rank Adaptation)  \n",
    "**Platform:** Google Colab (T4 GPU)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This comprehensive notebook contains the complete pipeline for building a medical question-answering LLM:\n",
    "\n",
    "1. **Data Exploration** - Analyze the medical dataset\n",
    "2. **Data Preprocessing** - Clean and format data for training\n",
    "3. **Model Fine-tuning** - Train TinyLlama with LoRA\n",
    "4. **Evaluation** - Compare base vs fine-tuned model\n",
    "5. **Inference Demo** - Test the model with real questions\n",
    "\n",
    "**Estimated Total Runtime:** 3-5 hours on Google Colab T4 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Section 1: Data Exploration](#section-1)\n",
    "- [Section 2: Data Preprocessing](#section-2)\n",
    "- [Section 3: Model Fine-tuning](#section-3)\n",
    "- [Section 4: Evaluation](#section-4)\n",
    "- [Section 5: Inference Demo](#section-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb0666",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Data Exploration <a id=\"section-1\"></a>\n",
    "\n",
    "In this section, we explore the **Medical Meadow Medical Flashcards** dataset to understand:\n",
    "- Dataset size and structure\n",
    "- Question and answer formats\n",
    "- Data quality and distribution\n",
    "- Sample medical content\n",
    "\n",
    "This helps us make informed decisions about preprocessing and training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709c1df2",
   "metadata": {},
   "source": [
    "## 1.1 Check GPU Availability\n",
    "\n",
    "**What this does:** Verifies that a CUDA-compatible GPU is available for training.\n",
    "\n",
    "**Why it's important:** Training language models requires GPU acceleration. Without a GPU, training would take days instead of hours.\n",
    "\n",
    "**Expected output:** Should show `CUDA available: True` on Google Colab with GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6542d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to Python path to import source modules\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import custom logger from source\n",
    "from src.utils.logger import setup_logger, ExperimentLogger\n",
    "\n",
    "# Create results folder structure\n",
    "base_dir = Path('results')\n",
    "folders = [\n",
    "    base_dir / 'visualizations',\n",
    "    base_dir / 'visualizations' / 'data_exploration',\n",
    "    base_dir / 'visualizations' / 'preprocessing',\n",
    "    base_dir / 'visualizations' / 'training',\n",
    "    base_dir / 'visualizations' / 'evaluation',\n",
    "    base_dir / 'visualizations' / 'inference',\n",
    "    base_dir / 'models',\n",
    "    base_dir / 'metrics',\n",
    "    base_dir / 'experiments'\n",
    "]\n",
    "\n",
    "# Create all folders\n",
    "for folder in folders:\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    print(f'✓ Created: {folder}')\n",
    "\n",
    "# Setup experiment logger using source module\n",
    "experiment_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "experiment_logger = ExperimentLogger(\n",
    "    experiment_name=f'medical_llm_{experiment_id}',\n",
    "    log_dir=str(base_dir / 'experiments')\n",
    ")\n",
    "\n",
    "print(f'\\n Experiment ID: {experiment_id}')\n",
    "print(f' Results will be saved to: {base_dir.absolute()}')\n",
    "print(f' Logging initialized with ExperimentLogger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226e511",
   "metadata": {},
   "source": [
    "## 1.0 Setup Results Folder Structure\n",
    "\n",
    "**What this does:** Creates organized folders to save all visualizations, models, and results.\n",
    "\n",
    "**Folder structure:**\n",
    "- `results/` - Main results directory\n",
    "- `results/visualizations/` - All plots and charts\n",
    "- `results/models/` - Saved models\n",
    "- `results/metrics/` - Evaluation metrics and tables\n",
    "- `results/experiments/` - Experiment tracking logs\n",
    "\n",
    "**Why it's important:** Organized file structure makes it easy to find and share results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccfd6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch to check GPU availability\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "\n",
    "# If GPU is available, print GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Name: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20182ce7",
   "metadata": {},
   "source": [
    "## 1.2 Load the Medical Dataset\n",
    "\n",
    "**What this does:** Downloads the Medical Meadow Medical Flashcards dataset from HuggingFace.\n",
    "\n",
    "**Dataset details:**\n",
    "- **Name:** medalpaca/medical_meadow_medical_flashcards\n",
    "- **Total size:** 33,955 medical Q&A pairs\n",
    "- **Format:** Each example has 'input' (question) and 'output' (answer)\n",
    "- **Content:** Medical flashcards covering various medical topics\n",
    "\n",
    "**Why this dataset:** High-quality medical content suitable for fine-tuning medical Q&A models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DatasetLoader from source module\n",
    "from src.data_processing.loader import DatasetLoader\n",
    "\n",
    "# Initialize data loader with dataset name\n",
    "print('Loading Medical Meadow Medical Flashcards dataset...')\n",
    "data_loader = DatasetLoader(\n",
    "    dataset_name='medalpaca/medical_meadow_medical_flashcards'\n",
    ")\n",
    "\n",
    "# Load the dataset using the custom loader\n",
    "dataset = data_loader.load_dataset()\n",
    "\n",
    "# Display total number of examples in the dataset\n",
    "print(f'\\nTotal examples in dataset: {len(dataset[\"train\"]):,}')\n",
    "\n",
    "# Show dataset structure\n",
    "print(f'\\nDataset structure: {dataset}')\n",
    "\n",
    "# Log dataset info\n",
    "experiment_logger.logger.info(f'Dataset loaded: {data_loader.dataset_name}')\n",
    "experiment_logger.logger.info(f'Total examples: {len(dataset[\"train\"]):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097678c4",
   "metadata": {},
   "source": [
    "## 1.3 Examine Sample Data\n",
    "\n",
    "**What this does:** Displays a few sample questions and answers from the dataset.\n",
    "\n",
    "**Why it's important:** Understanding the format and quality of data helps us:\n",
    "- Verify data quality\n",
    "- Design appropriate preprocessing steps\n",
    "- Understand the complexity of medical content\n",
    "- Determine suitable prompt formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047161f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 3 sample examples from the dataset\n",
    "print('SAMPLE MEDICAL Q&A PAIRS')\n",
    "print('=' * 80)\n",
    "\n",
    "for i in range(3):\n",
    "    example = dataset['train'][i]\n",
    "    print(f'\\nExample {i+1}:')\n",
    "    print(f\"Question: {example['input']}\")\n",
    "    print(f\"Answer: {example['output']}\")\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde188bf",
   "metadata": {},
   "source": [
    "## 1.4 Analyze Data Statistics\n",
    "\n",
    "**What this does:** Calculates statistical properties of the dataset including:\n",
    "- Average question length\n",
    "- Average answer length\n",
    "- Distribution of text lengths\n",
    "\n",
    "**Why it's important:** This helps us choose appropriate:\n",
    "- Maximum sequence length for tokenization\n",
    "- Batch size for training\n",
    "- Model configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62137938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert dataset to pandas DataFrame for easier analysis\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Calculate text lengths\n",
    "df['question_length'] = df['input'].str.len()\n",
    "df['answer_length'] = df['output'].str.len()\n",
    "df['total_length'] = df['question_length'] + df['answer_length']\n",
    "\n",
    "# Display statistics\n",
    "print('DATASET STATISTICS')\n",
    "print('=' * 80)\n",
    "print(f'Total examples: {len(df):,}')\n",
    "print(f'\\nQuestion Length:')\n",
    "print(f'  Average: {df[\"question_length\"].mean():.1f} characters')\n",
    "print(f'  Median: {df[\"question_length\"].median():.1f} characters')\n",
    "print(f'  Max: {df[\"question_length\"].max()} characters')\n",
    "print(f'\\nAnswer Length:')\n",
    "print(f'  Average: {df[\"answer_length\"].mean():.1f} characters')\n",
    "print(f'  Median: {df[\"answer_length\"].median():.1f} characters')\n",
    "print(f'  Max: {df[\"answer_length\"].max()} characters')\n",
    "print(f'\\nTotal Length (Q+A):')\n",
    "print(f'  Average: {df[\"total_length\"].mean():.1f} characters')\n",
    "print(f'  95th percentile: {df[\"total_length\"].quantile(0.95):.1f} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40dcf7",
   "metadata": {},
   "source": [
    "## 1.5 Visualize Data Distribution\n",
    "\n",
    "**What this does:** Creates visualizations showing the distribution of text lengths.\n",
    "\n",
    "**Why it's important:** Visual analysis helps us:\n",
    "- Identify outliers (very long or short texts)\n",
    "- Choose appropriate max_length for tokenization\n",
    "- Understand data balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create visualization of text length distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot question length distribution\n",
    "axes[0].hist(df['question_length'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Question Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Question Length Distribution')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot answer length distribution\n",
    "axes[1].hist(df['answer_length'], bins=50, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Answer Length (characters)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Answer Length Distribution')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Plot total length distribution\n",
    "axes[2].hist(df['total_length'], bins=50, color='lightgreen', edgecolor='black')\n",
    "axes[2].set_xlabel('Total Length (characters)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Total Q+A Length Distribution')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Distribution plots generated successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Sample 5000 examples for analysis\n",
    "subset = dataset['train'].select(range(5000))\n",
    "\n",
    "# Calculate text lengths\n",
    "question_lengths = [len(ex['input'].split()) for ex in subset]\n",
    "answer_lengths = [len(ex['output'].split()) for ex in subset]\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Medical Dataset - Comprehensive Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Question Length Distribution\n",
    "axes[0, 0].hist(question_lengths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Number of Words', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Question Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axvline(np.mean(question_lengths), color='red', linestyle='--', \n",
    "                    label=f'Mean: {np.mean(question_lengths):.1f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Answer Length Distribution\n",
    "axes[0, 1].hist(answer_lengths, bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Number of Words', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 1].set_title('Answer Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axvline(np.mean(answer_lengths), color='red', linestyle='--', \n",
    "                    label=f'Mean: {np.mean(answer_lengths):.1f}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Question-Answer Length Correlation\n",
    "axes[1, 0].scatter(question_lengths, answer_lengths, alpha=0.3, s=10, color='green')\n",
    "axes[1, 0].set_xlabel('Question Length (words)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Answer Length (words)', fontsize=12)\n",
    "axes[1, 0].set_title('Question vs Answer Length Correlation', fontsize=14, fontweight='bold')\n",
    "# Add correlation coefficient\n",
    "corr = np.corrcoef(question_lengths, answer_lengths)[0, 1]\n",
    "axes[1, 0].text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                transform=axes[1, 0].transAxes, fontsize=11,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 4. Top 20 Most Common Words (Medical Terms)\n",
    "# Combine all text and extract words\n",
    "all_text = ' '.join([ex['input'] + ' ' + ex['output'] for ex in subset])\n",
    "# Remove common stopwords and extract medical terms\n",
    "words = re.findall(r'\\b[a-zA-Z]{4,}\\b', all_text.lower())\n",
    "word_freq = Counter(words).most_common(20)\n",
    "words_list, counts = zip(*word_freq)\n",
    "\n",
    "axes[1, 1].barh(range(len(words_list)), counts, color='purple', alpha=0.7)\n",
    "axes[1, 1].set_yticks(range(len(words_list)))\n",
    "axes[1, 1].set_yticklabels(words_list)\n",
    "axes[1, 1].set_xlabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Top 20 Most Common Words', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "save_path = 'results/visualizations/data_exploration/comprehensive_data_analysis.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f'✓ Saved: {save_path}')\n",
    "plt.show()\n",
    "\n",
    "# Create and save summary statistics table\n",
    "stats_df = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Samples',\n",
    "        'Avg Question Length (words)',\n",
    "        'Avg Answer Length (words)',\n",
    "        'Max Question Length',\n",
    "        'Max Answer Length',\n",
    "        'Min Question Length',\n",
    "        'Min Answer Length',\n",
    "        'Median Question Length',\n",
    "        'Median Answer Length',\n",
    "        'Std Dev Question Length',\n",
    "        'Std Dev Answer Length'\n",
    "    ],\n",
    "    'Value': [\n",
    "        len(subset),\n",
    "        f'{np.mean(question_lengths):.2f}',\n",
    "        f'{np.mean(answer_lengths):.2f}',\n",
    "        max(question_lengths),\n",
    "        max(answer_lengths),\n",
    "        min(question_lengths),\n",
    "        min(answer_lengths),\n",
    "        np.median(question_lengths),\n",
    "        np.median(answer_lengths),\n",
    "        f'{np.std(question_lengths):.2f}',\n",
    "        f'{np.std(answer_lengths):.2f}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Save statistics table\n",
    "stats_path = 'results/metrics/data_exploration_statistics.csv'\n",
    "stats_df.to_csv(stats_path, index=False)\n",
    "print(f'✓ Saved: {stats_path}')\n",
    "\n",
    "# Display the table\n",
    "print('\\n' + '='*50)\n",
    "print('DATA EXPLORATION STATISTICS')\n",
    "print('='*50)\n",
    "print(stats_df.to_string(index=False))\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d9683",
   "metadata": {},
   "source": [
    "## 1.6 Comprehensive Data Visualizations\n",
    "\n",
    "**What this does:** Creates detailed visualizations of the dataset to understand:\n",
    "- Text length distributions (questions and answers)\n",
    "- Word frequency analysis\n",
    "- Data quality metrics\n",
    "- Sample distributions\n",
    "\n",
    "**Visualizations saved:**\n",
    "- Question length histogram\n",
    "- Answer length histogram\n",
    "- Top 20 most common words (bar chart)\n",
    "- Question-Answer length correlation scatter plot\n",
    "- Summary statistics table\n",
    "\n",
    "**Why it's important:** Visual analysis helps identify data patterns, outliers, and quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360cc22",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Data Preprocessing <a id=\"section-2\"></a>\n",
    "\n",
    "In this section, we prepare the data for training:\n",
    "- Load and clean the dataset\n",
    "- Select a subset (5,000 examples) for efficient training\n",
    "- Format data in instruction-response format\n",
    "- Split into train/validation/test sets\n",
    "- Save processed data\n",
    "\n",
    "**Goal:** Create high-quality training data for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9e322",
   "metadata": {},
   "source": [
    "## 2.1 Import Required Libraries\n",
    "\n",
    "**What this does:** Imports all necessary libraries for data preprocessing.\n",
    "\n",
    "**Libraries used:**\n",
    "- `pandas`: Data manipulation and analysis\n",
    "- `datasets`: HuggingFace datasets library\n",
    "- `transformers`: For tokenizer (optional check)\n",
    "- `warnings`: Suppress warning messages for cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5eaddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699cb65",
   "metadata": {},
   "source": [
    "## 2.2 Load and Inspect Dataset\n",
    "\n",
    "**What this does:** Loads the dataset and converts it to a pandas DataFrame for easier manipulation.\n",
    "\n",
    "**Why DataFrame:** Pandas provides powerful tools for:\n",
    "- Data cleaning\n",
    "- Duplicate removal\n",
    "- Filtering\n",
    "- Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd01abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the medical flashcards dataset\n",
    "print('Loading dataset...')\n",
    "dataset = load_dataset('medalpaca/medical_meadow_medical_flashcards')\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "print(f'Total examples loaded: {len(df):,}')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "print(f'\\nFirst few rows:')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e689e",
   "metadata": {},
   "source": [
    "## 2.3 Clean the Data\n",
    "\n",
    "**What this does:** Removes duplicates and empty entries to ensure data quality.\n",
    "\n",
    "**Cleaning steps:**\n",
    "1. Remove duplicate question-answer pairs\n",
    "2. Remove entries with empty questions\n",
    "3. Remove entries with empty answers\n",
    "\n",
    "**Why it's important:** Clean data leads to better model performance and prevents overfitting on duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4db17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DataPreprocessor from source module\n",
    "from src.data_processing.preprocessor import DataPreprocessor\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor(\n",
    "    instruction_template=\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f'Original size: {len(df):,} examples')\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_clean = df.drop_duplicates()\n",
    "print(f'After removing duplicates: {len(df_clean):,} examples')\n",
    "\n",
    "# Remove rows with empty questions or answers AND clean text using preprocessor\n",
    "df_clean['input'] = df_clean['input'].apply(preprocessor.clean_text)\n",
    "df_clean['output'] = df_clean['output'].apply(preprocessor.clean_text)\n",
    "\n",
    "# Remove empty entries after cleaning\n",
    "df_clean = df_clean[\n",
    "    (df_clean['input'].str.len() > 0) & \n",
    "    (df_clean['output'].str.len() > 0)\n",
    "]\n",
    "\n",
    "print(f'After cleaning and removing empty entries: {len(df_clean):,} examples')\n",
    "print(f'\\nRemoved {len(df) - len(df_clean):,} low-quality examples')\n",
    "\n",
    "# Log preprocessing step\n",
    "experiment_logger.logger.info(f'Data cleaned: {len(df_clean):,} examples remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbacc86",
   "metadata": {},
   "source": [
    "## 2.4 Select Subset for Efficient Training\n",
    "\n",
    "**What this does:** Randomly selects 5,000 examples from the cleaned dataset.\n",
    "\n",
    "**Why 5,000 examples:**\n",
    "- Balances training quality with time constraints\n",
    "- Fits comfortably in Google Colab's free tier\n",
    "- Sufficient for effective fine-tuning with LoRA\n",
    "- Allows completion within 2-4 hours\n",
    "\n",
    "**Random seed (42):** Ensures reproducibility - same subset every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subset size\n",
    "SUBSET_SIZE = 1000\n",
    "\n",
    "# Randomly sample 10000 examples (with fixed random seed for reproducibility)\n",
    "df_subset = df_clean.sample(n=SUBSET_SIZE, random_state=42)\n",
    "\n",
    "print(f'Selected subset size: {len(df_subset):,} examples')\n",
    "print(f'Percentage of full dataset: {(len(df_subset) / len(df_clean) * 100):.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13724a0",
   "metadata": {},
   "source": [
    "## 2.5 Format as Instruction-Response\n",
    "\n",
    "**What this does:** Converts raw Q&A pairs into instruction-following format.\n",
    "\n",
    "**Format used:**\n",
    "```\n",
    "### Instruction:\n",
    "[Question]\n",
    "\n",
    "### Response:\n",
    "[Answer]\n",
    "```\n",
    "\n",
    "**Why this format:**\n",
    "- Explicitly separates instruction from response\n",
    "- Compatible with instruction-tuned models like TinyLlama\n",
    "- Helps model understand its role\n",
    "- Standard format for fine-tuning chat/instruction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f71c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DataPreprocessor to format instruction-response pairs\n",
    "def format_instruction(row):\n",
    "    \"\"\"Convert Q&A pair to instruction-response format using preprocessor\"\"\"\n",
    "    return preprocessor.format_instruction_response(\n",
    "        instruction=row['input'],\n",
    "        response=row['output']\n",
    "    )\n",
    "\n",
    "# Apply formatting to all rows\n",
    "df_subset['text'] = df_subset.apply(format_instruction, axis=1)\n",
    "\n",
    "# Display a sample formatted example\n",
    "print('SAMPLE FORMATTED EXAMPLE:')\n",
    "print('=' * 80)\n",
    "print(df_subset['text'].iloc[0])\n",
    "print('=' * 80)\n",
    "print(f'\\nTotal formatted examples: {len(df_subset):,}')\n",
    "\n",
    "# Log formatting step\n",
    "experiment_logger.logger.info(f'Data formatted: {len(df_subset):,} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce41efb",
   "metadata": {},
   "source": [
    "## 2.6 Split into Train/Validation/Test Sets\n",
    "\n",
    "**What this does:** Divides data into three sets for training, validation, and testing.\n",
    "\n",
    "**Split ratios:**\n",
    "- **Train (85%):** 4,250 examples - Used to train the model\n",
    "- **Validation (10%):** 500 examples - Monitor training progress, prevent overfitting\n",
    "- **Test (5%):** 250 examples - Final evaluation of model performance\n",
    "\n",
    "**Why this split:**\n",
    "- Train: Largest portion for learning patterns\n",
    "- Validation: Detect overfitting during training\n",
    "- Test: Unbiased evaluation of final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 85% train, 15% temporary (for val+test)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_subset, \n",
    "    test_size=0.15, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Split the 15% into validation (10%) and test (5%)\n",
    "# 0.33 of 15% ≈ 5% of total\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.33, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Display split statistics\n",
    "print('DATASET SPLIT COMPLETED')\n",
    "print('=' * 80)\n",
    "print(f'Train set:      {len(train_df):,} examples ({len(train_df)/len(df_subset)*100:.1f}%)')\n",
    "print(f'Validation set: {len(val_df):,} examples ({len(val_df)/len(df_subset)*100:.1f}%)')\n",
    "print(f'Test set:       {len(test_df):,} examples ({len(test_df)/len(df_subset)*100:.1f}%)')\n",
    "print(f'Total:          {len(train_df) + len(val_df) + len(test_df):,} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903f12e",
   "metadata": {},
   "source": [
    "## 2.7 Save Processed Data\n",
    "\n",
    "**What this does:** Saves the processed datasets to CSV files.\n",
    "\n",
    "**Files created:**\n",
    "- `../data/processed/train.csv`: Training data\n",
    "- `../data/processed/val.csv`: Validation data\n",
    "- `../data/processed/test.csv`: Test data\n",
    "\n",
    "**Why save to CSV:**\n",
    "- Can be loaded quickly without re-processing\n",
    "- Easy to inspect and share\n",
    "- Compatible with various tools\n",
    "- Serves as backup of processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8065c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it doesn't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save each split to CSV (only input and output columns)\n",
    "train_df[['input', 'output']].to_csv('../data/processed/train.csv', index=False)\n",
    "val_df[['input', 'output']].to_csv('../data/processed/val.csv', index=False)\n",
    "test_df[['input', 'output']].to_csv('../data/processed/test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to HuggingFace Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df[['text']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text']])\n",
    "\n",
    "print('DATASETS CONVERTED SUCCESSFULLY!')\n",
    "print('=' * 80)\n",
    "print(f'Train dataset:      {len(train_dataset):,} examples')\n",
    "print(f'Validation dataset: {len(val_dataset):,} examples')\n",
    "print(f'Test dataset:       {len(test_dataset):,} examples')\n",
    "print('=' * 80)\n",
    "print('\\nDataset objects ready for training!')\n",
    "\n",
    "# Log dataset conversion\n",
    "experiment_logger.logger.info(f'Datasets converted: train={len(train_dataset)}, val={len(val_dataset)}, test={len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2e907",
   "metadata": {},
   "source": [
    "## 2.8 Convert to HuggingFace Dataset Format\n",
    "\n",
    "**What this does:** Converts pandas DataFrames to HuggingFace Dataset objects.\n",
    "\n",
    "**Why this is needed:**\n",
    "- HuggingFace Datasets are optimized for ML workflows\n",
    "- Required format for tokenization and training\n",
    "- Enables efficient data loading and batching\n",
    "- Compatible with Transformers library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452463b",
   "metadata": {},
   "source": [
    "## 2.9 Visualize Data Splits\n",
    "\n",
    "**What this does:** Creates comprehensive visualizations of the train/validation/test split.\n",
    "\n",
    "**Visualizations created:**\n",
    "- **Pie chart:** Shows proportions of each split\n",
    "- **Bar chart:** Compares sample counts across splits\n",
    "\n",
    "**Outputs:**\n",
    "- `results/visualizations/preprocessing/data_split_analysis.png` - Split visualization\n",
    "- `results/metrics/preprocessing_summary.csv` - Summary statistics table\n",
    "\n",
    "**Why visualize:** Helps verify correct data splitting and provides documentation for reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae7fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Data Preprocessing - Train/Val/Test Split Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Get split sizes\n",
    "train_size = len(train_dataset)\n",
    "val_size = len(val_dataset)\n",
    "test_size = len(test_dataset)\n",
    "total_size = train_size + val_size + test_size\n",
    "\n",
    "# 1. Pie Chart - Split Proportions\n",
    "sizes = [train_size, val_size, test_size]\n",
    "labels = [f'Train\\n({train_size:,} samples)\\n{train_size/total_size*100:.1f}%',\n",
    "          f'Validation\\n({val_size:,} samples)\\n{val_size/total_size*100:.1f}%',\n",
    "          f'Test\\n({test_size:,} samples)\\n{test_size/total_size*100:.1f}%']\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "explode = (0.05, 0.05, 0.05)\n",
    "\n",
    "axes[0].pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "            autopct='', startangle=90, textprops={'fontsize': 11, 'weight': 'bold'})\n",
    "axes[0].set_title('Dataset Split Proportions', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Bar Chart - Split Comparison\n",
    "split_names = ['Train', 'Validation', 'Test']\n",
    "bars = axes[1].bar(split_names, sizes, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Dataset Split Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim(0, max(sizes) * 1.1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, size) in enumerate(zip(bars, sizes)):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{size:,}\\n({size/total_size*100:.1f}%)',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add grid for better readability\n",
    "axes[1].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "save_path = 'results/visualizations/preprocessing/data_split_analysis.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f'✓ Saved: {save_path}')\n",
    "plt.show()\n",
    "\n",
    "# Create preprocessing summary table\n",
    "prep_summary_df = pd.DataFrame({\n",
    "    'Split': ['Training', 'Validation', 'Test', 'Total'],\n",
    "    'Samples': [train_size, val_size, test_size, total_size],\n",
    "    'Percentage': [f'{train_size/total_size*100:.1f}%', \n",
    "                   f'{val_size/total_size*100:.1f}%',\n",
    "                   f'{test_size/total_size*100:.1f}%',\n",
    "                   '100.0%'],\n",
    "    'Purpose': [\n",
    "        'Model training and weight updates',\n",
    "        'Hyperparameter tuning and early stopping',\n",
    "        'Final unbiased model evaluation',\n",
    "        'Complete dataset'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Save preprocessing summary\n",
    "prep_path = 'results/metrics/preprocessing_summary.csv'\n",
    "prep_summary_df.to_csv(prep_path, index=False)\n",
    "print(f'✓ Saved: {prep_path}')\n",
    "\n",
    "# Display the table\n",
    "print('\\n' + '='*80)\n",
    "print('PREPROCESSING SUMMARY')\n",
    "print('='*80)\n",
    "print(prep_summary_df.to_string(index=False))\n",
    "print('='*80)\n",
    "\n",
    "# Display sample formatted data\n",
    "print('\\n' + '='*80)\n",
    "print('SAMPLE FORMATTED DATA (First Example)')\n",
    "print('='*80)\n",
    "print(train_dataset[0]['text'][:500] + '...')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce616d59",
   "metadata": {},
   "source": [
    "## 2.7 Preprocessing Visualizations\n",
    "\n",
    "**What this does:** Creates visualizations showing:\n",
    "- Train/Validation/Test split proportions (pie chart)\n",
    "- Data distribution across splits (bar chart)\n",
    "- Sample formatted data display\n",
    "\n",
    "**Visualizations saved:**\n",
    "- Data split pie chart\n",
    "- Split comparison bar chart\n",
    "- Preprocessing summary table\n",
    "\n",
    "**Why it's important:** Confirms proper data splitting and formatting for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13532b23",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Model Fine-tuning with LoRA <a id=\"section-3\"></a>\n",
    "\n",
    "In this section, we fine-tune TinyLlama-1.1B using LoRA (Low-Rank Adaptation).\n",
    "\n",
    "**What is LoRA:**\n",
    "- Efficient fine-tuning method that trains only a small subset of parameters\n",
    "- Reduces memory usage by ~70%\n",
    "- Maintains performance quality\n",
    "- Perfect for training on limited GPU resources (Google Colab)\n",
    "\n",
    "**Training time:** Approximately 2-4 hours on Colab T4 GPU\n",
    "\n",
    "**Key configurations:**\n",
    "- 8-bit quantization for memory efficiency\n",
    "- LoRA rank=16, alpha=32\n",
    "- 3 epochs of training\n",
    "- Batch size=2 with gradient accumulation=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86776044",
   "metadata": {},
   "source": [
    "## 3.1 Check GPU Availability\n",
    "\n",
    "**What this does:** Verifies GPU is available and displays GPU specifications.\n",
    "\n",
    "**Important:** This step is critical - without GPU, training will fail or take days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print('GPU AVAILABILITY CHECK')\n",
    "print('=' * 80)\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Name: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "    print('\\nGPU is ready for training!')\n",
    "else:\n",
    "    print('\\nWARNING: No GPU detected!')\n",
    "    print('Please enable GPU in Colab: Runtime -> Change runtime type -> GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ad9cf",
   "metadata": {},
   "source": [
    "## 3.2 Install Required Packages\n",
    "\n",
    "**What this does:** Installs the latest versions of essential libraries.\n",
    "\n",
    "**Libraries installed:**\n",
    "- `transformers`: HuggingFace library for LLMs\n",
    "- `peft`: Parameter-Efficient Fine-Tuning (includes LoRA)\n",
    "- `datasets`: For dataset handling\n",
    "- `accelerate`: For distributed training\n",
    "- `bitsandbytes`: For 8-bit quantization\n",
    "- `trl`: Transformer Reinforcement Learning (improved training)\n",
    "\n",
    "**Note:** The `-q` flag makes installation quiet (less verbose output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.35.0 peft>=0.7.0 datasets>=2.14.0 accelerate>=0.24.0 bitsandbytes>=0.41.0 trl>=0.7.0\n",
    "\n",
    "print('All required packages installed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c53945b",
   "metadata": {},
   "source": [
    "## 3.3 Use Preprocessed Data\n",
    "\n",
    "**What this does:** Reuses the datasets already created in Section 2.\n",
    "\n",
    "**Advantages:**\n",
    "- No duplicate data loading\n",
    "- Consistent datasets across sections\n",
    "- Saves time and memory\n",
    "- Already cleaned, formatted, and split\n",
    "\n",
    "**Datasets available:**\n",
    "- `train_dataset`: 4,250 examples (85%)\n",
    "- `val_dataset`: 500 examples (10%)\n",
    "- `test_dataset`: 250 examples (5%)\n",
    "\n",
    "**Note:** These datasets are already formatted in instruction-response format and ready for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88020e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the datasets already prepared in Section 2\n",
    "# No need to reload and reprocess the data!\n",
    "\n",
    "print('Using datasets from Section 2...')\n",
    "print(f'Training set:   {len(train_dataset):,} examples')\n",
    "print(f'Validation set: {len(val_dataset):,} examples') \n",
    "print(f'Test set:       {len(test_dataset):,} examples')\n",
    "\n",
    "print(f'\\nSample example:')\n",
    "print(train_dataset[0]['text'][:300] + '...')\n",
    "\n",
    "# Create aliases for consistency with rest of Section 3\n",
    "train_data = train_dataset\n",
    "val_data = val_dataset\n",
    "test_data = test_dataset\n",
    "\n",
    "print('\\nDatasets ready for model training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a564f8b6",
   "metadata": {},
   "source": [
    "## 3.6 Load Model and Tokenizer with 8-bit Quantization\n",
    "\n",
    "**What this does:** Loads TinyLlama model with memory-efficient 8-bit quantization.\n",
    "\n",
    "**Model:** TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "- Size: 1.1 billion parameters\n",
    "- Type: Chat-optimized causal language model\n",
    "- Why chosen: Fits in Colab's 15GB GPU memory with quantization\n",
    "\n",
    "**8-bit Quantization benefits:**\n",
    "- Reduces memory usage by ~50%\n",
    "- Minimal performance loss (~1%)\n",
    "- Enables training on limited GPU\n",
    "\n",
    "**Configuration:**\n",
    "- `load_in_8bit=True`: Use 8-bit precision\n",
    "- `bnb_8bit_use_double_quant=True`: Further memory optimization\n",
    "- `device_map='auto'`: Automatic GPU placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "# Check if model is already loaded\n",
    "if 'model' not in globals() or model is None:\n",
    "    # Configure 8-bit quantization for memory efficiency\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,                    # Use 8-bit precision\n",
    "        bnb_8bit_use_double_quant=True,       # Double quantization for more memory savings\n",
    "        bnb_8bit_quant_type='nf8',            # NormalFloat 8-bit quantization\n",
    "        bnb_8bit_compute_dtype=torch.float16  # Compute in float16 for speed\n",
    "    )\n",
    "\n",
    "    print('Loading model with 8-bit quantization...')\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',           # Automatically place on GPU\n",
    "        trust_remote_code=True       # Allow custom model code\n",
    "    )\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Set padding token (required for batch training)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print('\\nMODEL LOADED SUCCESSFULLY')\n",
    "    print('=' * 80)\n",
    "    print(f'Model: {MODEL_NAME}')\n",
    "    print(f'Total parameters: {model.num_parameters():,}')\n",
    "    print(f'Memory format: 8-bit quantized')\n",
    "    print(f'Device: {next(model.parameters()).device}')\n",
    "else:\n",
    "    print('MODEL ALREADY LOADED')\n",
    "    print('=' * 80)\n",
    "    print(f'Model: {MODEL_NAME}')\n",
    "    print(f'Total parameters: {model.num_parameters():,}')\n",
    "    print(f'Memory format: 8-bit quantized')\n",
    "    print(f'Device: {next(model.parameters()).device}')\n",
    "    print('\\nSkipping model loading to save time!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6d748",
   "metadata": {},
   "source": [
    "## 3.7 Configure LoRA (Low-Rank Adaptation)\n",
    "\n",
    "**What this does:** Sets up LoRA for efficient fine-tuning.\n",
    "\n",
    "**What is LoRA:**\n",
    "- Adds small trainable matrices (adapters) to model layers\n",
    "- Freezes original model weights\n",
    "- Trains only ~0.5% of total parameters\n",
    "- Results in much faster training and lower memory usage\n",
    "\n",
    "**LoRA Configuration:**\n",
    "- `r=16`: Rank of low-rank matrices (higher = more capacity, more memory)\n",
    "- `lora_alpha=32`: Scaling factor (typically 2x rank)\n",
    "- `lora_dropout=0.05`: Dropout for regularization\n",
    "- `target_modules`: Which attention layers to adapt\n",
    "  - `q_proj`: Query projection\n",
    "  - `k_proj`: Key projection\n",
    "  - `v_proj`: Value projection\n",
    "  - `o_proj`: Output projection\n",
    "\n",
    "**Why these modules:** Attention layers contain most of the model's knowledge and adapt well to new tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for k-bit (8-bit) training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                                           # Low-rank dimension\n",
    "    lora_alpha=32,                                  # Scaling factor (2 * r)\n",
    "    lora_dropout=0.05,                              # Dropout probability\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],  # Attention layers to adapt\n",
    "    bias='none',                                    # Don't adapt bias terms\n",
    "    task_type='CAUSAL_LM'                          # Task: Causal Language Modeling\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Display trainable parameters\n",
    "print('LORA CONFIGURATION APPLIED')\n",
    "print('=' * 80)\n",
    "model.print_trainable_parameters()\n",
    "print('\\nOnly these LoRA adapters will be trained!')\n",
    "print('Original model weights remain frozen.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3000530",
   "metadata": {},
   "source": [
    "## 3.8 Tokenize Dataset\n",
    "\n",
    "**What this does:** Converts text into token IDs that the model can process.\n",
    "\n",
    "**Tokenization process:**\n",
    "1. Convert text to token IDs using tokenizer\n",
    "2. Truncate sequences longer than 512 tokens\n",
    "3. Pad shorter sequences to 512 tokens\n",
    "4. Create labels (copy of input_ids for causal LM)\n",
    "\n",
    "**Parameters:**\n",
    "- `max_length=512`: Maximum sequence length\n",
    "  - Chosen based on dataset statistics (95th percentile)\n",
    "  - Balances context vs memory usage\n",
    "- `padding='max_length'`: Pad all sequences to same length for efficient batching\n",
    "- `truncation=True`: Cut off sequences exceeding max_length\n",
    "\n",
    "**Batched processing:** Processes multiple examples at once for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42471b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text examples for training.\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of examples with 'text' field\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with input_ids, attention_mask, and labels\n",
    "    \"\"\"\n",
    "    # Tokenize text\n",
    "    outputs = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,          # Truncate long sequences\n",
    "        max_length=512,           # Maximum sequence length\n",
    "        padding='max_length',     # Pad to max_length\n",
    "        return_tensors=None       # Return Python lists\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    outputs['labels'] = outputs['input_ids'].copy()\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "print('Tokenizing datasets...')\n",
    "\n",
    "# Tokenize training data\n",
    "train_tokenized = train_data.map(\n",
    "    tokenize_function, \n",
    "    batched=True,                          # Process in batches for speed\n",
    "    remove_columns=train_data.column_names  # Remove all original columns after tokenization\n",
    ")\n",
    "\n",
    "# Tokenize validation data\n",
    "val_tokenized = val_data.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=val_data.column_names\n",
    ")\n",
    "\n",
    "print('\\nTOKENIZATION COMPLETE')\n",
    "print('=' * 80)\n",
    "print(f'Tokenized training examples:   {len(train_tokenized):,}')\n",
    "print(f'Tokenized validation examples: {len(val_tokenized):,}')\n",
    "print(f'Sequence length: {train_tokenized[0][\"input_ids\"].__len__()} tokens')\n",
    "print('\\nDatasets ready for training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277d093",
   "metadata": {},
   "source": [
    "## 3.9 Configure Training Parameters\n",
    "\n",
    "**What this does:** Sets all hyperparameters for the training process.\n",
    "\n",
    "**Key parameters explained:**\n",
    "\n",
    "**Training length:**\n",
    "- `num_train_epochs=1`: Train for 1 complete pass through data\n",
    "  - Faster training for development and testing\n",
    "  - Can increase to 2-3 epochs for final production model\n",
    "\n",
    "**Batch sizes:**\n",
    "- `per_device_train_batch_size=2`: Process 2 examples at once\n",
    "- `gradient_accumulation_steps=4`: Accumulate gradients over 4 batches\n",
    "- **Effective batch size = 2 × 4 = 8**\n",
    "  - Small batch size fits in limited GPU memory\n",
    "  - Gradient accumulation simulates larger batch\n",
    "\n",
    "**Optimization:**\n",
    "- `learning_rate=2e-4`: Step size for weight updates\n",
    "  - Higher than typical (1e-5) because we're using LoRA\n",
    "  - LoRA allows higher learning rates\n",
    "- `lr_scheduler_type='cosine'`: Gradually decrease learning rate\n",
    "- `warmup_steps=100`: Slowly increase LR at start\n",
    "\n",
    "**Precision:**\n",
    "- `fp16=True`: Use 16-bit floating point\n",
    "  - 2x faster training\n",
    "  - 2x less memory\n",
    "  - Negligible accuracy loss\n",
    "\n",
    "**Checkpointing:**\n",
    "- `save_steps=500`: Save model every 500 steps\n",
    "- `eval_steps=500`: Evaluate on validation set every 500 steps\n",
    "- `load_best_model_at_end=True`: Keep best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118256c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training configuration\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir='./medical_llm_checkpoints',   # Where to save checkpoints\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=1,                       # Number of training epochs\n",
    "    \n",
    "    # Batch sizes\n",
    "    per_device_train_batch_size=2,            # Batch size per GPU for training\n",
    "    per_device_eval_batch_size=2,             # Batch size per GPU for evaluation\n",
    "    gradient_accumulation_steps=4,            # Accumulate gradients over 4 batches\n",
    "                                              # Effective batch size = 2 * 4 = 8\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=2e-4,                       # Learning rate (higher for LoRA)\n",
    "    lr_scheduler_type='cosine',               # Cosine learning rate schedule\n",
    "    warmup_steps=100,                         # Warmup steps for learning rate\n",
    "    \n",
    "    # Precision\n",
    "    fp16=True,                                # Use mixed precision training\n",
    "    \n",
    "    # Checkpointing and evaluation\n",
    "    save_steps=500,                           # Save checkpoint every 500 steps\n",
    "    eval_steps=500,                           # Evaluate every 500 steps\n",
    "    logging_steps=50,                         # Log metrics every 50 steps\n",
    "    logging_first_step=True,                  # Log from the very first step\n",
    "    eval_strategy='steps',                    # Evaluate at regular intervals\n",
    "    save_strategy='steps',                    # Save at regular intervals\n",
    "    load_best_model_at_end=True,             # Load best checkpoint at end\n",
    "    \n",
    "    # Logging\n",
    "    report_to='none',                         # Don't use external logging (wandb, etc.)\n",
    "    disable_tqdm=False,                       # Keep progress bars enabled\n",
    "    logging_strategy='steps'                  # Log at regular step intervals\n",
    ")\n",
    "\n",
    "print('TRAINING CONFIGURATION SET')\n",
    "print('=' * 80)\n",
    "print(f'Epochs: {training_args.num_train_epochs}')\n",
    "print(f'Batch size per device: {training_args.per_device_train_batch_size}')\n",
    "print(f'Gradient accumulation: {training_args.gradient_accumulation_steps}')\n",
    "print(f'Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}')\n",
    "print(f'Learning rate: {training_args.learning_rate}')\n",
    "print(f'Mixed precision: {training_args.fp16}')\n",
    "print(f'\\nEstimated training time: 2-4 hours on Colab T4 GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210f130",
   "metadata": {},
   "source": [
    "## 3.10 Train the Model\n",
    "\n",
    "**What this does:** Runs the actual training process.\n",
    "\n",
    "**Training process:**\n",
    "1. Initialize Trainer with model, data, and configuration\n",
    "2. Start training loop:\n",
    "   - Forward pass: Generate predictions\n",
    "   - Calculate loss: Compare with actual answers\n",
    "   - Backward pass: Compute gradients\n",
    "   - Update LoRA weights\n",
    "3. Periodically evaluate on validation set\n",
    "4. Save checkpoints\n",
    "\n",
    "**What you'll see:**\n",
    "- Progress bar showing steps completed\n",
    "- Training loss (should decrease over time)\n",
    "- Validation loss (should also decrease)\n",
    "- Time per batch\n",
    "\n",
    "**Expected behavior:**\n",
    "- Training loss should steadily decrease\n",
    "- Validation loss should decrease then plateau\n",
    "- If validation loss increases, model may be overfitting\n",
    "\n",
    "**Duration:** Approximately 2-4 hours for 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom callback to display epoch progress\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class EpochProgressCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EPOCH {int(state.epoch) + 1}/{int(args.num_train_epochs)} STARTING\")\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EPOCH {int(state.epoch)}/{int(args.num_train_epochs)} COMPLETED\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Initialize Trainer with callback\n",
    "trainer = Trainer(\n",
    "    model=model,                    # LoRA-enhanced model\n",
    "    args=training_args,             # Training configuration\n",
    "    train_dataset=train_tokenized,  # Tokenized training data\n",
    "    eval_dataset=val_tokenized,     # Tokenized validation data\n",
    "    callbacks=[EpochProgressCallback()]  # Add epoch progress callback\n",
    ")\n",
    "\n",
    "print('STARTING TRAINING')\n",
    "print('=' * 80)\n",
    "print('This will take approximately 2-4 hours.')\n",
    "print('You can monitor progress below:')\n",
    "print('  - Progress bars show step-by-step progress')\n",
    "print('  - Epoch transitions will be clearly marked')\n",
    "print('  - Loss metrics logged every 50 steps')\n",
    "print('  - Evaluation runs every 500 steps')\n",
    "print('Loss should decrease over time.')\n",
    "print('=' * 80)\n",
    "print()\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('TRAINING COMPLETE!')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295efa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training metrics from trainer state\n",
    "import json\n",
    "\n",
    "# Get training history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract loss values\n",
    "train_losses = [log['loss'] for log in log_history if 'loss' in log]\n",
    "train_steps = [log['step'] for log in log_history if 'loss' in log]\n",
    "\n",
    "# Create comprehensive training visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Training Progress - Comprehensive Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Training Loss Curve\n",
    "axes[0, 0].plot(train_steps, train_losses, linewidth=2, color='blue', marker='o', markersize=4)\n",
    "axes[0, 0].set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Training Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, linestyle='--')\n",
    "# Add annotations for first and last loss\n",
    "if train_losses:\n",
    "    axes[0, 0].annotate(f'Start: {train_losses[0]:.4f}', \n",
    "                        xy=(train_steps[0], train_losses[0]),\n",
    "                        xytext=(10, 10), textcoords='offset points',\n",
    "                        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5),\n",
    "                        fontsize=10)\n",
    "    axes[0, 0].annotate(f'End: {train_losses[-1]:.4f}', \n",
    "                        xy=(train_steps[-1], train_losses[-1]),\n",
    "                        xytext=(-60, 10), textcoords='offset points',\n",
    "                        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5),\n",
    "                        fontsize=10)\n",
    "\n",
    "# 2. Learning Rate Schedule\n",
    "learning_rates = [log['learning_rate'] for log in log_history if 'learning_rate' in log]\n",
    "lr_steps = [log['step'] for log in log_history if 'learning_rate' in log]\n",
    "if learning_rates:\n",
    "    axes[0, 1].plot(lr_steps, learning_rates, linewidth=2, color='red', marker='s', markersize=4)\n",
    "    axes[0, 1].set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3, linestyle='--')\n",
    "    axes[0, 1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "# 3. Loss Reduction Percentage\n",
    "if len(train_losses) > 1:\n",
    "    loss_reduction = [(train_losses[0] - loss) / train_losses[0] * 100 for loss in train_losses]\n",
    "    axes[1, 0].plot(train_steps, loss_reduction, linewidth=2, color='green', marker='^', markersize=4)\n",
    "    axes[1, 0].set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Loss Reduction (%)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_title('Cumulative Loss Reduction', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3, linestyle='--')\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. Training Statistics Summary (Text Box)\n",
    "axes[1, 1].axis('off')\n",
    "if train_losses:\n",
    "    initial_loss = train_losses[0]\n",
    "    final_loss = train_losses[-1]\n",
    "    avg_loss = np.mean(train_losses)\n",
    "    min_loss = min(train_losses)\n",
    "    loss_improvement = ((initial_loss - final_loss) / initial_loss) * 100\n",
    "    \n",
    "    stats_text = f'''\n",
    "    TRAINING STATISTICS\n",
    "    {'='*40}\n",
    "    \n",
    "    Initial Loss:     {initial_loss:.6f}\n",
    "    Final Loss:       {final_loss:.6f}\n",
    "    Average Loss:     {avg_loss:.6f}\n",
    "    Minimum Loss:     {min_loss:.6f}\n",
    "    \n",
    "    Loss Improvement: {loss_improvement:.2f}%\n",
    "    Total Steps:      {train_steps[-1]:,}\n",
    "    Total Epochs:     {training_args.num_train_epochs}\n",
    "    \n",
    "    Model:            TinyLlama-1.1B-Chat\n",
    "    Method:           LoRA Fine-tuning\n",
    "    Samples:          {len(train_dataset):,}\n",
    "    \n",
    "    LoRA Config:\n",
    "      - Rank (r):     {peft_config.r}\n",
    "      - Alpha:        {peft_config.lora_alpha}\n",
    "      - Dropout:      {peft_config.lora_dropout}\n",
    "    \n",
    "    Training Args:\n",
    "      - Batch Size:   {training_args.per_device_train_batch_size}\n",
    "      - Learning Rate: {training_args.learning_rate}\n",
    "      - Gradient Acc:  {training_args.gradient_accumulation_steps}\n",
    "    '''\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.5, stats_text, fontsize=11, family='monospace',\n",
    "                    verticalalignment='center',\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save training visualization\n",
    "save_path = 'results/visualizations/training/training_progress.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f'✓ Saved: {save_path}')\n",
    "plt.show()\n",
    "\n",
    "# Create and save training metrics table\n",
    "if train_losses:\n",
    "    training_metrics_df = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Initial Loss',\n",
    "            'Final Loss',\n",
    "            'Average Loss',\n",
    "            'Minimum Loss',\n",
    "            'Loss Improvement (%)',\n",
    "            'Total Steps',\n",
    "            'Total Epochs',\n",
    "            'Training Samples',\n",
    "            'Batch Size',\n",
    "            'Learning Rate',\n",
    "            'Gradient Accumulation',\n",
    "            'LoRA Rank',\n",
    "            'LoRA Alpha',\n",
    "            'LoRA Dropout'\n",
    "        ],\n",
    "        'Value': [\n",
    "            f'{initial_loss:.6f}',\n",
    "            f'{final_loss:.6f}',\n",
    "            f'{avg_loss:.6f}',\n",
    "            f'{min_loss:.6f}',\n",
    "            f'{loss_improvement:.2f}%',\n",
    "            train_steps[-1],\n",
    "            training_args.num_train_epochs,\n",
    "            len(train_dataset),\n",
    "            training_args.per_device_train_batch_size,\n",
    "            training_args.learning_rate,\n",
    "            training_args.gradient_accumulation_steps,\n",
    "            peft_config.r,\n",
    "            peft_config.lora_alpha,\n",
    "            peft_config.lora_dropout\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = 'results/metrics/training_metrics.csv'\n",
    "    training_metrics_df.to_csv(metrics_path, index=False)\n",
    "    print(f' Saved: {metrics_path}')\n",
    "    \n",
    "    # Display table\n",
    "    print('\\n' + '='*60)\n",
    "    print('TRAINING METRICS SUMMARY')\n",
    "    print('='*60)\n",
    "    print(training_metrics_df.to_string(index=False))\n",
    "    print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef92dac",
   "metadata": {},
   "source": [
    "## 3.14 Training Metrics Tracking and Visualization\n",
    "\n",
    "**What this does:** Creates comprehensive visualizations of the training process:\n",
    "- Training loss curve over time\n",
    "- Learning rate schedule\n",
    "- Training progress metrics\n",
    "- GPU memory usage over time (if available)\n",
    "\n",
    "**Visualizations saved:**\n",
    "- Training loss curve\n",
    "- Learning rate schedule\n",
    "- Training metrics table\n",
    "- Experiment configuration table\n",
    "\n",
    "**Why it's important:** Visualizing training helps identify:\n",
    "- If the model is learning (loss decreasing)\n",
    "- If there's overfitting\n",
    "- If hyperparameters are appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b7119",
   "metadata": {},
   "source": [
    "## 3.11 Save the Fine-tuned Model\n",
    "\n",
    "**What this does:** Saves the trained LoRA adapters and tokenizer to disk.\n",
    "\n",
    "**What gets saved:**\n",
    "- LoRA adapter weights (~100-200 MB)\n",
    "- Adapter configuration\n",
    "- Tokenizer files\n",
    "\n",
    "**What doesn't get saved:**\n",
    "- Base model (TinyLlama) - will be loaded separately\n",
    "- Only the LoRA adapters are saved (much smaller)\n",
    "\n",
    "**Location:** `./medical_llm_final/`\n",
    "\n",
    "**Important:** Download this folder before Colab session ends!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156336ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "save_directory = './medical_llm_final'\n",
    "\n",
    "print('Saving model...')\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print('\\nMODEL SAVED SUCCESSFULLY')\n",
    "print('=' * 80)\n",
    "print(f'Location: {save_directory}/')\n",
    "print('\\nFiles saved:')\n",
    "print('  - adapter_config.json   (LoRA configuration)')\n",
    "print('  - adapter_model.bin     (LoRA weights)')\n",
    "print('  - tokenizer files')\n",
    "print('\\nIMPORTANT: Download this folder before ending Colab session!')\n",
    "print('You can also upload to Google Drive or HuggingFace Hub.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b0533",
   "metadata": {},
   "source": [
    "## 3.12 Test the Fine-tuned Model\n",
    "\n",
    "**What this does:** Generates a test response to verify the model works.\n",
    "\n",
    "**Test question:** \"What are the symptoms of diabetes?\"\n",
    "\n",
    "**What to expect:**\n",
    "- Model should generate a medically accurate response\n",
    "- Response should be relevant to diabetes symptoms\n",
    "- Quality may not be perfect but should show medical knowledge\n",
    "\n",
    "**Parameters:**\n",
    "- `max_new_tokens=100`: Generate up to 100 new tokens\n",
    "- `temperature=0.7`: Balance between creativity and focus\n",
    "  - Lower (0.1) = more focused, deterministic\n",
    "  - Higher (1.0) = more creative, diverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect available device (GPU or CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Test the model with a sample question\n",
    "test_question = 'What are the symptoms of diabetes?'\n",
    "\n",
    "# Format in instruction format\n",
    "input_text = f\"\"\"### Instruction:\n",
    "{test_question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate response\n",
    "print('TEST GENERATION')\n",
    "print('=' * 80)\n",
    "print(f'Question: {test_question}')\n",
    "print('\\nGenerating response...')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=100,     # Generate up to 100 tokens\n",
    "        temperature=0.7,        # Sampling temperature\n",
    "        do_sample=True,         # Use sampling instead of greedy\n",
    "        top_p=0.9               # Nucleus sampling\n",
    "    )\n",
    "\n",
    "# Decode response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print('\\nGenerated Response:')\n",
    "print('-' * 80)\n",
    "print(response)\n",
    "print('=' * 80)\n",
    "print('\\nModel is working! Proceed to evaluation for detailed metrics.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966fc53",
   "metadata": {},
   "source": [
    "## 3.13 Training Complete - Next Steps\n",
    "\n",
    "**Congratulations! Model fine-tuning is complete.**\n",
    "\n",
    "**What you've accomplished:**\n",
    "- ✓ Fine-tuned TinyLlama on 5,000 medical Q&A pairs\n",
    "- ✓ Used LoRA for efficient training\n",
    "- ✓ Saved model checkpoints\n",
    "- ✓ Verified model generates responses\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "1. **Download Model:**\n",
    "   - Right-click on `medical_llm_final` folder\n",
    "   - Select \"Download\"\n",
    "   - Or upload to Google Drive/HuggingFace Hub\n",
    "\n",
    "2. **Run Evaluation (Section 4):**\n",
    "   - Calculate BLEU and ROUGE scores\n",
    "   - Compare with base model\n",
    "   - Generate metrics report\n",
    "\n",
    "3. **Test Inference (Section 5):**\n",
    "   - Try various medical questions\n",
    "   - Experiment with parameters\n",
    "   - Build demo interface\n",
    "\n",
    "4. **Deploy:**\n",
    "   - Use Gradio interface\n",
    "   - Deploy FastAPI backend\n",
    "   - Build React UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c4ee5",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Model Evaluation <a id=\"section-4\"></a>\n",
    "\n",
    "In this section, we evaluate the fine-tuned model's performance:\n",
    "\n",
    "**Evaluation approach:**\n",
    "- Compare base TinyLlama vs fine-tuned model\n",
    "- Calculate quantitative metrics (BLEU, ROUGE)\n",
    "- Perform qualitative analysis\n",
    "- Visualize improvements\n",
    "\n",
    "**Metrics used:**\n",
    "- **BLEU:** Measures n-gram overlap between generated and reference text\n",
    "- **ROUGE-1:** Unigram overlap\n",
    "- **ROUGE-L:** Longest common subsequence\n",
    "\n",
    "**Expected results:**\n",
    "- Fine-tuned model should outperform base model\n",
    "- Higher BLEU/ROUGE scores = better quality\n",
    "- Qualitative improvements in medical accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d3bfac",
   "metadata": {},
   "source": [
    "## 4.1 Import Libraries and Setup\n",
    "\n",
    "**What this does:** Imports required libraries for evaluation.\n",
    "\n",
    "**Libraries:**\n",
    "- `torch`: For model inference\n",
    "- `transformers`: Load models\n",
    "- `peft`: Load LoRA adapters\n",
    "- `evaluate`: HuggingFace metrics library\n",
    "- `pandas`: Data handling\n",
    "\n",
    "**Device:** Uses GPU if available, CPU otherwise (GPU much faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39726101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from evaluate import load\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('EVALUATION SETUP')\n",
    "print('=' * 80)\n",
    "print(f'Device: {device}')\n",
    "print('Libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e093d",
   "metadata": {},
   "source": [
    "## 4.2 Load Base Model\n",
    "\n",
    "**What this does:** Loads the original TinyLlama model without fine-tuning.\n",
    "\n",
    "**Purpose:** Establish baseline performance to compare against fine-tuned model.\n",
    "\n",
    "**Configuration:**\n",
    "- No quantization (for fair comparison)\n",
    "- Float16 precision for speed\n",
    "- Automatic device placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "print('Loading base model...')\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print('\\nBASE MODEL LOADED')\n",
    "print('=' * 80)\n",
    "print(f'Model: {MODEL_NAME}')\n",
    "print(f'Parameters: {base_model.num_parameters():,}')\n",
    "print('This is the ORIGINAL model without medical fine-tuning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650f367",
   "metadata": {},
   "source": [
    "## 4.3 Load Fine-tuned Model\n",
    "\n",
    "**What this does:** Loads the fine-tuned model with LoRA adapters.\n",
    "\n",
    "**Process:**\n",
    "1. Load base TinyLlama model\n",
    "2. Load LoRA adapters from saved directory\n",
    "3. Merge adapters with base model\n",
    "\n",
    "**merge_and_unload():** Combines LoRA adapters with base weights for faster inference.\n",
    "\n",
    "**Location:** Loads from `../models/final/medical_llm_final` or `./medical_llm_final`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading fine-tuned model...')\n",
    "\n",
    "# Load base model again\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load LoRA adapters\n",
    "finetuned_model = PeftModel.from_pretrained(\n",
    "    finetuned_model,\n",
    "    './medical_llm_final'  # Adjust path if needed\n",
    ")\n",
    "\n",
    "# Merge adapters for faster inference\n",
    "finetuned_model = finetuned_model.merge_and_unload()\n",
    "\n",
    "print('\\nFINE-TUNED MODEL LOADED')\n",
    "print('=' * 80)\n",
    "print('LoRA adapters merged with base model.')\n",
    "print('This model has been trained on medical Q&A data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563f862",
   "metadata": {},
   "source": [
    "## 4.4 Load Test Dataset\n",
    "\n",
    "**What this does:** Loads the test set for evaluation.\n",
    "\n",
    "**Test set:**\n",
    "- 250 medical Q&A pairs (5% of total)\n",
    "- Never seen during training\n",
    "- Provides unbiased performance estimate\n",
    "\n",
    "**Subset selection:** Evaluating all 250 takes time, so we evaluate 50 samples.\n",
    "- Still statistically significant\n",
    "- Balances thoroughness with speed\n",
    "- Can evaluate full 250 if time permits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae856cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv('../data/processed/test.csv')\n",
    "\n",
    "# Select 50 samples for evaluation (adjust as needed)\n",
    "test_questions = test_df['input'].tolist()[:50]\n",
    "references = test_df['output'].tolist()[:50]\n",
    "\n",
    "print('TEST DATA LOADED')\n",
    "print('=' * 80)\n",
    "print(f'Total test examples: {len(test_df)}')\n",
    "print(f'Evaluating on: {len(test_questions)} samples')\n",
    "print(f'\\nSample question: {test_questions[0]}')\n",
    "print(f'Reference answer: {references[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1463f6",
   "metadata": {},
   "source": [
    "## 4.5 Generate Predictions\n",
    "\n",
    "**What this does:** Generates responses from both models for all test questions.\n",
    "\n",
    "**Process:**\n",
    "1. For each test question:\n",
    "   - Format as instruction prompt\n",
    "   - Generate response from base model\n",
    "   - Generate response from fine-tuned model\n",
    "2. Collect all predictions\n",
    "\n",
    "**Generation parameters:**\n",
    "- `max_new_tokens=150`: Limit response length\n",
    "- `temperature=0.7`: Balance between focused and creative\n",
    "- `do_sample=True`: Use sampling for variety\n",
    "\n",
    "**Duration:** Takes 5-10 minutes for 50 samples (faster with GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b82c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, question):\n",
    "    \"\"\"\n",
    "    Generate a response to a medical question.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use for generation\n",
    "        question: The medical question string\n",
    "    \n",
    "    Returns:\n",
    "        Generated response string\n",
    "    \"\"\"\n",
    "    # Format prompt\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the response part\n",
    "    if '### Response:' in response:\n",
    "        response = response.split('### Response:')[1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print('GENERATING PREDICTIONS')\n",
    "print('=' * 80)\n",
    "print('This will take 5-10 minutes...')\n",
    "print()\n",
    "\n",
    "# Generate predictions from base model\n",
    "print('Generating base model predictions...')\n",
    "base_predictions = [generate_response(base_model, q) for q in test_questions]\n",
    "\n",
    "# Generate predictions from fine-tuned model\n",
    "print('Generating fine-tuned model predictions...')\n",
    "finetuned_predictions = [generate_response(finetuned_model, q) for q in test_questions]\n",
    "\n",
    "print('\\nPREDICTIONS COMPLETE!')\n",
    "print('=' * 80)\n",
    "print(f'Base model predictions: {len(base_predictions)}')\n",
    "print(f'Fine-tuned predictions: {len(finetuned_predictions)}')\n",
    "print('Ready for metric calculation!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526afc8",
   "metadata": {},
   "source": [
    "## 4.6 Calculate Evaluation Metrics\n",
    "\n",
    "**What this does:** Computes BLEU and ROUGE scores for both models.\n",
    "\n",
    "**Metrics explained:**\n",
    "\n",
    "**BLEU (Bilingual Evaluation Understudy):**\n",
    "- Measures n-gram overlap between generated and reference text\n",
    "- Range: 0.0 (worst) to 1.0 (perfect match)\n",
    "- Higher = more similar to reference\n",
    "- Good for measuring factual accuracy\n",
    "\n",
    "**ROUGE-1 (Recall-Oriented Understudy for Gisting Evaluation):**\n",
    "- Measures unigram (single word) overlap\n",
    "- Range: 0.0 to 1.0\n",
    "- Checks if important words are present\n",
    "\n",
    "**ROUGE-L:**\n",
    "- Measures longest common subsequence\n",
    "- Captures sentence-level structure\n",
    "- Better for evaluating fluency\n",
    "\n",
    "**Expected improvement:** Fine-tuned model should score 10-30% higher on all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "bleu = load('bleu')\n",
    "rouge = load('rouge')\n",
    "\n",
    "# Debug: Check predictions before calculating metrics\n",
    "print('=' * 80)\n",
    "print('PREDICTION DEBUGGING')\n",
    "print('=' * 80)\n",
    "print(f'Number of test questions: {len(test_questions)}')\n",
    "print(f'Number of references: {len(references)}')\n",
    "print(f'Number of base predictions: {len(base_predictions)}')\n",
    "print(f'Number of finetuned predictions: {len(finetuned_predictions)}')\n",
    "print(f'\\nSample check (first 3 examples):')\n",
    "for i in range(min(3, len(test_questions))):\n",
    "    print(f'\\n--- Example {i+1} ---')\n",
    "    print(f'Question: {test_questions[i][:100]}...')\n",
    "    print(f'Reference length: {len(references[i])} chars')\n",
    "    print(f'Base pred length: {len(base_predictions[i])} chars')\n",
    "    print(f'Finetuned pred length: {len(finetuned_predictions[i])} chars')\n",
    "    print(f'Base prediction: {base_predictions[i][:150]}...')\n",
    "    print(f'Finetuned prediction: {finetuned_predictions[i][:150]}...')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('Calculating metrics...')\n",
    "\n",
    "# Calculate BLEU scores\n",
    "base_bleu = bleu.compute(\n",
    "    predictions=base_predictions, \n",
    "    references=[[r] for r in references]  # BLEU expects list of references\n",
    ")\n",
    "finetuned_bleu = bleu.compute(\n",
    "    predictions=finetuned_predictions, \n",
    "\n",
    "    references=[[r] for r in references]\n",
    ")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "base_rouge = rouge.compute(\n",
    "    predictions=base_predictions, \n",
    "    references=references\n",
    ")\n",
    "finetuned_rouge = rouge.compute(\n",
    "    predictions=finetuned_predictions, \n",
    "    references=references\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print('\\n' + '=' * 80)\n",
    "print('EVALUATION RESULTS')\n",
    "print('=' * 80)\n",
    "\n",
    "# Helper function to calculate percentage improvement safely\n",
    "def calculate_improvement_pct(base_val, finetuned_val):\n",
    "    if base_val == 0:\n",
    "        if finetuned_val == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float('inf')  # Infinite improvement from 0\n",
    "    return ((finetuned_val - base_val) / base_val * 100)\n",
    "\n",
    "print('\\nBLEU Score:')\n",
    "print(f'  Base Model:       {base_bleu[\"bleu\"]:.4f}')\n",
    "print(f'  Fine-tuned Model: {finetuned_bleu[\"bleu\"]:.4f}')\n",
    "bleu_diff = finetuned_bleu[\"bleu\"] - base_bleu[\"bleu\"]\n",
    "bleu_pct = calculate_improvement_pct(base_bleu[\"bleu\"], finetuned_bleu[\"bleu\"])\n",
    "if bleu_pct == float('inf'):\n",
    "    print(f'  Improvement:      +{bleu_diff:.4f} (∞% - base was 0)')\n",
    "else:\n",
    "    print(f'  Improvement:      +{bleu_diff:.4f} ({bleu_pct:.1f}%)')\n",
    "\n",
    "print('\\nROUGE-1 (Unigram Overlap):')\n",
    "print(f'  Base Model:       {base_rouge[\"rouge1\"]:.4f}')\n",
    "print(f'  Fine-tuned Model: {finetuned_rouge[\"rouge1\"]:.4f}')\n",
    "rouge1_diff = finetuned_rouge[\"rouge1\"] - base_rouge[\"rouge1\"]\n",
    "rouge1_pct = calculate_improvement_pct(base_rouge[\"rouge1\"], finetuned_rouge[\"rouge1\"])\n",
    "if rouge1_pct == float('inf'):\n",
    "    print(f'  Improvement:      +{rouge1_diff:.4f} (∞% - base was 0)')\n",
    "else:\n",
    "    print(f'  Improvement:      +{rouge1_diff:.4f} ({rouge1_pct:.1f}%)')\n",
    "\n",
    "print('\\nROUGE-L (Longest Common Subsequence):')\n",
    "print(f'  Base Model:       {base_rouge[\"rougeL\"]:.4f}')\n",
    "print(f'  Fine-tuned Model: {finetuned_rouge[\"rougeL\"]:.4f}')\n",
    "rougeL_diff = finetuned_rouge[\"rougeL\"] - base_rouge[\"rougeL\"]\n",
    "rougeL_pct = calculate_improvement_pct(base_rouge[\"rougeL\"], finetuned_rouge[\"rougeL\"])\n",
    "if rougeL_pct == float('inf'):\n",
    "    print(f'  Improvement:      +{rougeL_diff:.4f} (∞% - base was 0)')\n",
    "else:\n",
    "    print(f'  Improvement:      +{rougeL_diff:.4f} ({rougeL_pct:.1f}%)')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "# Prepare metrics dictionaries for comprehensive visualization\n",
    "base_metrics = {\n",
    "    'bleu': base_bleu['bleu'],\n",
    "    'bleu_1': base_bleu.get('precisions', [0, 0, 0, 0])[0],\n",
    "    'bleu_2': base_bleu.get('precisions', [0, 0, 0, 0])[1],\n",
    "    'bleu_3': base_bleu.get('precisions', [0, 0, 0, 0])[2],\n",
    "    'bleu_4': base_bleu.get('precisions', [0, 0, 0, 0])[3],\n",
    "    'rouge_1': base_rouge['rouge1'],\n",
    "    'rouge_2': base_rouge['rouge2'],\n",
    "    'rouge_l': base_rouge['rougeL']\n",
    "}\n",
    "\n",
    "finetuned_metrics = {\n",
    "    'bleu': finetuned_bleu['bleu'],\n",
    "    'bleu_1': finetuned_bleu.get('precisions', [0, 0, 0, 0])[0],\n",
    "    'bleu_2': finetuned_bleu.get('precisions', [0, 0, 0, 0])[1],\n",
    "    'bleu_3': finetuned_bleu.get('precisions', [0, 0, 0, 0])[2],\n",
    "    'bleu_4': finetuned_bleu.get('precisions', [0, 0, 0, 0])[3],\n",
    "    'rouge_1': finetuned_rouge['rouge1'],\n",
    "    'rouge_2': finetuned_rouge['rouge2'],\n",
    "    'rouge_l': finetuned_rouge['rougeL']\n",
    "}\n",
    "\n",
    "print('\\n✓ Metrics dictionaries created for visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive evaluation visualizations\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "fig.suptitle('Model Evaluation - Base vs Fine-tuned Comparison', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Prepare data\n",
    "metrics_names = list(base_metrics.keys())\n",
    "base_values = [base_metrics[m] for m in metrics_names]\n",
    "finetuned_values = [finetuned_metrics[m] for m in metrics_names]\n",
    "improvements = [((ft - base) / base * 100) if base > 0 else 0 \n",
    "                for base, ft in zip(base_values, finetuned_values)]\n",
    "\n",
    "# Color palette\n",
    "colors_base = '#ff6b6b'\n",
    "colors_ft = '#4ecdc4'\n",
    "colors_improvement = '#95e1d3'\n",
    "\n",
    "# 1. Overall Metrics Comparison (Bar Chart)\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, base_values, width, label='Base Model', \n",
    "                color=colors_base, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "bars2 = ax1.bar(x + width/2, finetuned_values, width, label='Fine-tuned Model',\n",
    "                color=colors_ft, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax1.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Base vs Fine-tuned Model - All Metrics', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([m.upper().replace('_', '-') for m in metrics_names], rotation=45, ha='right')\n",
    "ax1.legend(fontsize=11, loc='upper left')\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. Improvement Percentage (Horizontal Bar Chart)\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "colors_imp = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "bars = ax2.barh(range(len(metrics_names)), improvements, color=colors_imp, alpha=0.7, edgecolor='black')\n",
    "ax2.set_yticks(range(len(metrics_names)))\n",
    "ax2.set_yticklabels([m.upper().replace('_', '-') for m in metrics_names], fontsize=10)\n",
    "ax2.set_xlabel('Improvement (%)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Metric Improvements', fontsize=12, fontweight='bold')\n",
    "ax2.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (bar, imp) in enumerate(zip(bars, improvements)):\n",
    "    width = bar.get_width()\n",
    "    label_x = width + (1 if width > 0 else -1)\n",
    "    ax2.text(label_x, bar.get_y() + bar.get_height()/2,\n",
    "            f'{imp:+.1f}%', ha='left' if width > 0 else 'right', \n",
    "            va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 3. BLEU Scores Detailed Comparison\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "bleu_metrics = [m for m in metrics_names if 'bleu' in m.lower()]\n",
    "bleu_base = [base_metrics[m] for m in bleu_metrics]\n",
    "bleu_ft = [finetuned_metrics[m] for m in bleu_metrics]\n",
    "\n",
    "x_bleu = np.arange(len(bleu_metrics))\n",
    "ax3.plot(x_bleu, bleu_base, marker='o', linewidth=2, markersize=8, \n",
    "         label='Base Model', color=colors_base)\n",
    "ax3.plot(x_bleu, bleu_ft, marker='s', linewidth=2, markersize=8,\n",
    "         label='Fine-tuned', color=colors_ft)\n",
    "ax3.set_xticks(x_bleu)\n",
    "ax3.set_xticklabels([m.replace('bleu_', 'BLEU-').upper() for m in bleu_metrics])\n",
    "ax3.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('BLEU Scores Comparison', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# 4. ROUGE Scores Detailed Comparison\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "rouge_metrics = [m for m in metrics_names if 'rouge' in m.lower()]\n",
    "rouge_base = [base_metrics[m] for m in rouge_metrics]\n",
    "rouge_ft = [finetuned_metrics[m] for m in rouge_metrics]\n",
    "\n",
    "x_rouge = np.arange(len(rouge_metrics))\n",
    "ax4.plot(x_rouge, rouge_base, marker='o', linewidth=2, markersize=8,\n",
    "         label='Base Model', color=colors_base)\n",
    "ax4.plot(x_rouge, rouge_ft, marker='s', linewidth=2, markersize=8,\n",
    "         label='Fine-tuned', color=colors_ft)\n",
    "ax4.set_xticks(x_rouge)\n",
    "ax4.set_xticklabels([m.replace('rouge_', 'ROUGE-').replace('_', '-').upper() for m in rouge_metrics],\n",
    "                     rotation=45, ha='right')\n",
    "ax4.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('ROUGE Scores Comparison', fontsize=12, fontweight='bold')\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# 5. Average Score Comparison (Gauge-style)\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "avg_base = np.mean(base_values)\n",
    "avg_ft = np.mean(finetuned_values)\n",
    "avg_improvement = ((avg_ft - avg_base) / avg_base * 100) if avg_base > 0 else 0\n",
    "\n",
    "categories = ['Base\\nModel', 'Fine-tuned\\nModel']\n",
    "averages = [avg_base, avg_ft]\n",
    "colors_avg = [colors_base, colors_ft]\n",
    "\n",
    "bars = ax5.bar(categories, averages, color=colors_avg, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax5.set_ylabel('Average Score', fontsize=11, fontweight='bold')\n",
    "ax5.set_title('Overall Average Performance', fontsize=12, fontweight='bold')\n",
    "ax5.set_ylim(0, max(averages) * 1.2)\n",
    "ax5.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "for bar, avg in zip(bars, averages):\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{avg:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add improvement annotation\n",
    "ax5.text(0.5, max(averages) * 1.1, f'Improvement: {avg_improvement:+.2f}%',\n",
    "        ha='center', fontsize=11, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# 6. Summary Statistics Table\n",
    "ax6 = fig.add_subplot(gs[2, :])\n",
    "ax6.axis('tight')\n",
    "ax6.axis('off')\n",
    "\n",
    "table_data = []\n",
    "table_data.append(['Metric', 'Base Model', 'Fine-tuned Model', 'Improvement', 'Status'])\n",
    "for i, metric in enumerate(metrics_names):\n",
    "    status = '✓ Better' if improvements[i] > 0 else '✗ Worse' if improvements[i] < 0 else '= Same'\n",
    "    status_color = 'lightgreen' if improvements[i] > 0 else 'lightcoral' if improvements[i] < 0 else 'lightyellow'\n",
    "    table_data.append([\n",
    "        metric.upper().replace('_', '-'),\n",
    "        f'{base_values[i]:.4f}',\n",
    "        f'{finetuned_values[i]:.4f}',\n",
    "        f'{improvements[i]:+.2f}%',\n",
    "        status\n",
    "    ])\n",
    "\n",
    "# Add average row\n",
    "table_data.append(['AVERAGE', f'{avg_base:.4f}', f'{avg_ft:.4f}', \n",
    "                   f'{avg_improvement:+.2f}%', '✓ Better' if avg_improvement > 0 else '✗ Worse'])\n",
    "\n",
    "table = ax6.table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                  colWidths=[0.25, 0.15, 0.15, 0.15, 0.15])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style the header row\n",
    "for i in range(5):\n",
    "    table[(0, i)].set_facecolor('#40466e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Style data rows\n",
    "for i in range(1, len(table_data)):\n",
    "    for j in range(5):\n",
    "        if i == len(table_data) - 1:  # Average row\n",
    "            table[(i, j)].set_facecolor('#fffacd')\n",
    "            table[(i, j)].set_text_props(weight='bold')\n",
    "        elif j == 4:  # Status column\n",
    "            if 'Better' in table_data[i][4]:\n",
    "                table[(i, j)].set_facecolor('lightgreen')\n",
    "            elif 'Worse' in table_data[i][4]:\n",
    "                table[(i, j)].set_facecolor('lightcoral')\n",
    "        else:\n",
    "            table[(i, j)].set_facecolor('white' if i % 2 == 0 else '#f0f0f0')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save comprehensive evaluation visualization\n",
    "save_path = 'results/visualizations/evaluation/comprehensive_evaluation.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f'✓ Saved: {save_path}')\n",
    "plt.show()\n",
    "\n",
    "# Create and save detailed evaluation results table\n",
    "eval_results_df = pd.DataFrame({\n",
    "    'Metric': [m.upper().replace('_', '-') for m in metrics_names],\n",
    "    'Base_Model': base_values,\n",
    "    'Finetuned_Model': finetuned_values,\n",
    "    'Improvement_Percent': improvements,\n",
    "    'Improvement_Absolute': [ft - base for base, ft in zip(base_values, finetuned_values)]\n",
    "})\n",
    "\n",
    "# Add summary row\n",
    "summary_row = pd.DataFrame({\n",
    "    'Metric': ['AVERAGE'],\n",
    "    'Base_Model': [avg_base],\n",
    "    'Finetuned_Model': [avg_ft],\n",
    "    'Improvement_Percent': [avg_improvement],\n",
    "    'Improvement_Absolute': [avg_ft - avg_base]\n",
    "})\n",
    "eval_results_df = pd.concat([eval_results_df, summary_row], ignore_index=True)\n",
    "\n",
    "# Save results\n",
    "results_path = 'results/metrics/evaluation_results.csv'\n",
    "eval_results_df.to_csv(results_path, index=False)\n",
    "print(f'✓ Saved: {results_path}')\n",
    "\n",
    "# Display table\n",
    "print('\\n' + '='*90)\n",
    "print('COMPREHENSIVE EVALUATION RESULTS')\n",
    "print('='*90)\n",
    "print(eval_results_df.to_string(index=False))\n",
    "print('='*90)\n",
    "print(f'\\n Overall Improvement: {avg_improvement:+.2f}%')\n",
    "print(f' Best Improvement: {max(improvements):.2f}% ({metrics_names[improvements.index(max(improvements))].upper()})')\n",
    "print('='*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf083b",
   "metadata": {},
   "source": [
    "## 4.5 Comprehensive Evaluation Visualizations\n",
    "\n",
    "**What this does:** Creates detailed comparison visualizations:\n",
    "- Base vs Fine-tuned model metrics (bar charts)\n",
    "- BLEU and ROUGE score comparisons\n",
    "- Metric improvement percentages\n",
    "- Side-by-side response quality comparison\n",
    "- Per-metric breakdown\n",
    "\n",
    "**Visualizations saved:**\n",
    "- Model comparison charts (multiple views)\n",
    "- Metrics breakdown radar chart\n",
    "- Improvement analysis\n",
    "- Detailed evaluation results table\n",
    "\n",
    "**Why it's important:** Visual comparison clearly shows the fine-tuning impact and helps justify the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1b2df",
   "metadata": {},
   "source": [
    "## 4.7 Visualize Metric Comparison\n",
    "\n",
    "**What this does:** Creates a bar chart comparing base vs fine-tuned model performance.\n",
    "\n",
    "**Visualization benefits:**\n",
    "- Easy to see improvements at a glance\n",
    "- Suitable for reports and presentations\n",
    "- Clearly shows fine-tuned model advantage\n",
    "\n",
    "**Chart details:**\n",
    "- Blue bars: Base model\n",
    "- Orange bars: Fine-tuned model\n",
    "- Y-axis: Score (0-1 scale)\n",
    "- X-axis: Different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dca965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-L']\n",
    "base_scores = [\n",
    "    base_bleu['bleu'], \n",
    "    base_rouge['rouge1'], \n",
    "    base_rouge['rougeL']\n",
    "]\n",
    "finetuned_scores = [\n",
    "    finetuned_bleu['bleu'], \n",
    "    finetuned_rouge['rouge1'], \n",
    "    finetuned_rouge['rougeL']\n",
    "]\n",
    "\n",
    "# Create bar chart\n",
    "x = range(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot bars\n",
    "bars1 = ax.bar(\n",
    "    [i - width/2 for i in x], \n",
    "    base_scores, \n",
    "    width, \n",
    "    label='Base Model', \n",
    "    alpha=0.8,\n",
    "    color='skyblue'\n",
    ")\n",
    "bars2 = ax.bar(\n",
    "    [i + width/2 for i in x], \n",
    "    finetuned_scores, \n",
    "    width, \n",
    "    label='Fine-tuned Model', \n",
    "    alpha=0.8,\n",
    "    color='lightcoral'\n",
    ")\n",
    "\n",
    "# Customize chart\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, max(max(base_scores), max(finetuned_scores)) * 1.1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width()/2., \n",
    "            height,\n",
    "            f'{height:.3f}',\n",
    "            ha='center', \n",
    "            va='bottom',\n",
    "            fontsize=9\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../evaluation/comparisons/metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Visualization saved to: ../evaluation/comparisons/metrics_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e05c5",
   "metadata": {},
   "source": [
    "## 4.8 Qualitative Comparison - Example Responses\n",
    "\n",
    "**What this does:** Shows side-by-side comparison of responses from both models.\n",
    "\n",
    "**Purpose:**\n",
    "- Metrics don't tell the whole story\n",
    "- Human evaluation is crucial\n",
    "- Shows qualitative improvements in:\n",
    "  - Medical accuracy\n",
    "  - Response relevance\n",
    "  - Completeness\n",
    "  - Clarity\n",
    "\n",
    "**What to look for:**\n",
    "- Fine-tuned model should give more medically accurate answers\n",
    "- Responses should be more focused on the question\n",
    "- Better use of medical terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354571a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('QUALITATIVE COMPARISON - SAMPLE RESPONSES')\n",
    "print('=' * 80)\n",
    "\n",
    "# Determine how many examples to display (up to 3)\n",
    "num_examples = min(3, len(test_questions), len(references), len(base_predictions), len(finetuned_predictions))\n",
    "\n",
    "if num_examples == 0:\n",
    "    print('\\nNo predictions available. Please run the prediction generation cells first.')\n",
    "else:\n",
    "    # Display example comparisons\n",
    "    for i in range(num_examples):\n",
    "        print(f'\\nExample {i+1}:')\n",
    "        print('-' * 80)\n",
    "        print(f'QUESTION: {test_questions[i]}')\n",
    "        print()\n",
    "        print(f'REFERENCE ANSWER:')\n",
    "        print(f'{references[i]}')\n",
    "        print()\n",
    "        print(f'BASE MODEL RESPONSE:')\n",
    "        print(f'{base_predictions[i]}')\n",
    "        print()\n",
    "        print(f'FINE-TUNED MODEL RESPONSE:')\n",
    "        print(f'{finetuned_predictions[i]}')\n",
    "        print('=' * 80)\n",
    "\n",
    "    print('\\nQualitative Analysis:')\n",
    "    print('- Compare medical accuracy of responses')\n",
    "    print('- Fine-tuned model should be more relevant and focused')\n",
    "    print('- Better use of medical terminology')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da497d9b",
   "metadata": {},
   "source": [
    "## 4.9 Save Evaluation Results\n",
    "\n",
    "**What this does:** Saves all metrics to a JSON file for later reference.\n",
    "\n",
    "**File:** `../evaluation/metrics/evaluation_results.json`\n",
    "\n",
    "**Contents:**\n",
    "- BLEU scores (base and fine-tuned)\n",
    "- ROUGE-1 scores\n",
    "- ROUGE-L scores\n",
    "\n",
    "**Use cases:**\n",
    "- Include in project report\n",
    "- Compare with future training runs\n",
    "- Track model improvements over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74dc7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Prepare results dictionary\n",
    "results = {\n",
    "    'base_bleu': float(base_bleu['bleu']),\n",
    "    'finetuned_bleu': float(finetuned_bleu['bleu']),\n",
    "    'bleu_improvement': float(finetuned_bleu['bleu'] - base_bleu['bleu']),\n",
    "    'base_rouge1': float(base_rouge['rouge1']),\n",
    "    'finetuned_rouge1': float(finetuned_rouge['rouge1']),\n",
    "    'rouge1_improvement': float(finetuned_rouge['rouge1'] - base_rouge['rouge1']),\n",
    "    'base_rougeL': float(base_rouge['rougeL']),\n",
    "    'finetuned_rougeL': float(finetuned_rouge['rougeL']),\n",
    "    'rougeL_improvement': float(finetuned_rouge['rougeL'] - base_rouge['rougeL']),\n",
    "    'num_test_samples': len(test_questions)\n",
    "}\n",
    "\n",
    "# Create directory\n",
    "os.makedirs('../evaluation/metrics', exist_ok=True)\n",
    "\n",
    "# Save to JSON\n",
    "with open('../evaluation/metrics/evaluation_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print('RESULTS SAVED')\n",
    "print('=' * 80)\n",
    "print('File: ../evaluation/metrics/evaluation_results.json')\n",
    "print('\\nContents:')\n",
    "print(json.dumps(results, indent=2))\n",
    "print('\\nEvaluation complete! Results ready for report.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9173e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example predictions comparison visualization\n",
    "from textwrap import wrap\n",
    "\n",
    "# Select diverse test examples based on available data\n",
    "dataset_size = len(test_dataset)\n",
    "num_examples = min(5, dataset_size)  # Up to 5 examples, or less if dataset is smaller\n",
    "\n",
    "if dataset_size == 0:\n",
    "    print('WARNING: test_dataset is empty. Cannot generate example comparisons.')\n",
    "    example_comparisons = []\n",
    "else:\n",
    "    # Generate evenly spaced indices across the dataset\n",
    "    if num_examples == 1:\n",
    "        example_indices = [0]\n",
    "    else:\n",
    "        step = (dataset_size - 1) // (num_examples - 1)\n",
    "        example_indices = [i * step for i in range(num_examples)]\n",
    "        # Ensure last index is within bounds\n",
    "        example_indices[-1] = min(example_indices[-1], dataset_size - 1)\n",
    "    \n",
    "    print(f'Dataset size: {dataset_size}')\n",
    "    print(f'Selected {num_examples} examples at indices: {example_indices}')\n",
    "    \n",
    "    examples = [test_dataset[i] for i in example_indices]\n",
    "\n",
    "    # Generate predictions\n",
    "    print('Generating predictions for comparison...')\n",
    "    example_comparisons = []\n",
    "\n",
    "    for idx, example in enumerate(examples):\n",
    "        # Extract question\n",
    "        question = example['text'].split('### Response:')[0].replace('### Instruction:', '').strip()\n",
    "        expected = example['text'].split('### Response:')[1].strip() if '### Response:' in example['text'] else ''\n",
    "        \n",
    "        # Generate from base model\n",
    "        inputs_base = tokenizer(question, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs_base = base_model.generate(**inputs_base, max_new_tokens=100, do_sample=False)\n",
    "        response_base = tokenizer.decode(outputs_base[0], skip_special_tokens=True).replace(question, '').strip()\n",
    "        \n",
    "        # Generate from fine-tuned model\n",
    "        inputs_ft = tokenizer(question, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs_ft = model.generate(**inputs_ft, max_new_tokens=100, do_sample=False)\n",
    "        response_ft = tokenizer.decode(outputs_ft[0], skip_special_tokens=True).replace(question, '').strip()\n",
    "        \n",
    "        example_comparisons.append({\n",
    "            'question': question[:100] + '...' if len(question) > 100 else question,\n",
    "            'expected': expected[:100] + '...' if len(expected) > 100 else expected,\n",
    "            'base': response_base[:100] + '...' if len(response_base) > 100 else response_base,\n",
    "            'finetuned': response_ft[:100] + '...' if len(response_ft) > 100 else response_ft\n",
    "        })\n",
    "\n",
    "    print(f' Generated {len(example_comparisons)} example comparisons')\n",
    "\n",
    "# Only create visualizations if we have examples\n",
    "if len(example_comparisons) > 0:\n",
    "    # Create comparison table visualization\n",
    "    fig, axes = plt.subplots(len(example_comparisons), 1, figsize=(18, 5 * len(example_comparisons)))\n",
    "    if len(example_comparisons) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    fig.suptitle('Example Predictions - Base vs Fine-tuned Model Comparison', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "    for idx, (ax, comparison) in enumerate(zip(axes, example_comparisons)):\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create table data\n",
    "        table_data = [\n",
    "            ['Component', 'Content'],\n",
    "            ['Question', '\\n'.join(wrap(comparison['question'], width=80))],\n",
    "            ['Expected Answer', '\\n'.join(wrap(comparison['expected'], width=80))],\n",
    "            ['Base Model', '\\n'.join(wrap(comparison['base'], width=80))],\n",
    "            ['Fine-tuned Model', '\\n'.join(wrap(comparison['finetuned'], width=80))]\n",
    "        ]\n",
    "        \n",
    "        table = ax.table(cellText=table_data, cellLoc='left', loc='center',\n",
    "                         colWidths=[0.2, 0.8])\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1, 3)\n",
    "        \n",
    "        # Style the table\n",
    "        table[(0, 0)].set_facecolor('#40466e')\n",
    "        table[(0, 1)].set_facecolor('#40466e')\n",
    "        table[(0, 0)].set_text_props(weight='bold', color='white')\n",
    "        table[(0, 1)].set_text_props(weight='bold', color='white')\n",
    "        \n",
    "        colors = ['#e8f4f8', '#fff9e6', '#ffe6e6', '#e6ffe6']\n",
    "        for i in range(1, 5):\n",
    "            table[(i, 0)].set_facecolor('#f0f0f0')\n",
    "            table[(i, 0)].set_text_props(weight='bold')\n",
    "            table[(i, 1)].set_facecolor(colors[i-1])\n",
    "        \n",
    "        ax.set_title(f'Example {idx + 1}', fontsize=12, fontweight='bold', pad=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save examples visualization\n",
    "    save_path = 'results/visualizations/evaluation/example_predictions.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f'✓ Saved: {save_path}')\n",
    "    plt.show()\n",
    "\n",
    "    # Save examples as CSV\n",
    "    examples_df = pd.DataFrame(example_comparisons)\n",
    "    examples_df.index = [f'Example_{i+1}' for i in range(len(examples_df))]\n",
    "    examples_path = 'results/metrics/example_predictions.csv'\n",
    "    examples_df.to_csv(examples_path)\n",
    "    print(f'✓ Saved: {examples_path}')\n",
    "else:\n",
    "    print('No examples to visualize.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1987533",
   "metadata": {},
   "source": [
    "## 4.6 Example Predictions Visualization\n",
    "\n",
    "**What this does:** Creates side-by-side comparison of model responses:\n",
    "- Shows actual medical questions\n",
    "- Displays expected answers\n",
    "- Shows base model responses\n",
    "- Shows fine-tuned model responses\n",
    "- Highlights quality improvements\n",
    "\n",
    "**Visualizations saved:**\n",
    "- Example predictions comparison table\n",
    "- Response quality analysis\n",
    "\n",
    "**Why it's important:** Real examples demonstrate the practical improvement in response quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c894b5d7",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Interactive Inference Demo <a id=\"section-5\"></a>\n",
    "\n",
    "In this section, we demonstrate the fine-tuned model's capabilities:\n",
    "\n",
    "**Features:**\n",
    "- Load fine-tuned model\n",
    "- Test with various medical questions\n",
    "- Experiment with generation parameters\n",
    "- Explore different medical domains\n",
    "- Interactive mode for custom questions\n",
    "\n",
    "**Goal:** Showcase the model's practical medical Q&A abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9769c1",
   "metadata": {},
   "source": [
    "## 5.1 Setup and Load Model\n",
    "\n",
    "**What this does:** Loads the fine-tuned model for interactive testing.\n",
    "\n",
    "**Setup:**\n",
    "- Import required libraries\n",
    "- Detect available device (GPU/CPU)\n",
    "- Load tokenizer and model\n",
    "- Merge LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Detect device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('INFERENCE DEMO SETUP')\n",
    "print('=' * 80)\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load model\n",
    "MODEL_NAME = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    './medical_llm_final'\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print('\\nModel loaded successfully!')\n",
    "print('Ready for medical Q&A!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00349f01",
   "metadata": {},
   "source": [
    "## 5.2 Define Generation Function\n",
    "\n",
    "**What this does:** Creates a reusable function for asking medical questions.\n",
    "\n",
    "**Function parameters:**\n",
    "- `question`: The medical question string\n",
    "- `temperature`: Controls randomness (0.1-1.0)\n",
    "  - Lower (0.3) = More focused, deterministic\n",
    "  - Higher (1.0) = More creative, diverse\n",
    "- `max_length`: Maximum response length in tokens\n",
    "\n",
    "**Generation settings:**\n",
    "- `top_p=0.9`: Nucleus sampling (consider top 90% probability mass)\n",
    "- `do_sample=True`: Use sampling instead of greedy decoding\n",
    "- `repetition_penalty=1.1`: Penalize repeated phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96636940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_medical_question(question, temperature=0.7, max_length=200):\n",
    "    \"\"\"\n",
    "    Generate a response to a medical question.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The medical question\n",
    "        temperature (float): Sampling temperature (0.1-1.0)\n",
    "            - Lower = more focused and deterministic\n",
    "            - Higher = more creative and diverse\n",
    "        max_length (int): Maximum response length in tokens\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated medical response\n",
    "    \"\"\"\n",
    "    # Format prompt\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,                # Nucleus sampling\n",
    "            do_sample=True,            # Enable sampling\n",
    "            repetition_penalty=1.1     # Reduce repetition\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the response portion\n",
    "    if '### Response:' in response:\n",
    "        response = response.split('### Response:')[1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print('Generation function ready!')\n",
    "print('\\nUsage: ask_medical_question(\"Your question?\", temperature=0.7, max_length=200)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5330f6",
   "metadata": {},
   "source": [
    "## 5.3 Test with Common Medical Questions\n",
    "\n",
    "**What this does:** Tests the model with 5 common medical questions.\n",
    "\n",
    "**Test questions cover:**\n",
    "1. Diabetes symptoms\n",
    "2. Hypertension treatment\n",
    "3. Aspirin side effects\n",
    "4. Migraine causes\n",
    "5. Insulin mechanism\n",
    "\n",
    "**Purpose:** Demonstrate model's ability to answer diverse medical questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    'What are the symptoms of diabetes?',\n",
    "    'How is hypertension treated?',\n",
    "    'What are the side effects of aspirin?',\n",
    "    'What causes migraine headaches?',\n",
    "    'How does insulin work?'\n",
    "]\n",
    "\n",
    "print('MEDICAL Q&A DEMONSTRATIONS')\n",
    "print('=' * 80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f'\\nQuestion {i}: {question}')\n",
    "    print('-' * 80)\n",
    "    \n",
    "    response = ask_medical_question(question)\n",
    "    \n",
    "    print(f'Answer: {response}')\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9722604",
   "metadata": {},
   "source": [
    "## 5.4 Parameter Experimentation - Temperature\n",
    "\n",
    "**What this does:** Shows how temperature affects response quality.\n",
    "\n",
    "**Temperature values tested:**\n",
    "- **0.3:** More focused, factual, deterministic responses\n",
    "- **0.7:** Balanced between creativity and accuracy\n",
    "- **1.0:** More creative, diverse, but potentially less accurate\n",
    "\n",
    "**Observation:** Lower temperature is typically better for medical questions where accuracy is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12aa287",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q = 'What are the symptoms of heart disease?'\n",
    "\n",
    "print('PARAMETER EXPERIMENTATION - Temperature')\n",
    "print('=' * 80)\n",
    "print(f'Question: {test_q}')\n",
    "print('\\nTesting different temperature values...')\n",
    "print()\n",
    "\n",
    "for temp in [0.3, 0.7, 1.0]:\n",
    "    print(f'\\nTemperature: {temp}')\n",
    "    print('-' * 80)\n",
    "    response = ask_medical_question(test_q, temperature=temp)\n",
    "    print(f'Response: {response}')\n",
    "    print('=' * 80)\n",
    "\n",
    "print('\\nObservation:')\n",
    "print('- Lower temperature (0.3): More focused, factual')\n",
    "print('- Medium temperature (0.7): Balanced')\n",
    "print('- Higher temperature (1.0): More creative, diverse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive experiment tracking report\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print('='*80)\n",
    "print('CREATING COMPREHENSIVE EXPERIMENT TRACKING REPORT')\n",
    "print('='*80)\n",
    "\n",
    "# Get GPU info safely\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = f'{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}'\n",
    "else:\n",
    "    gpu_name = 'None (CPU only)'\n",
    "    gpu_memory = 'N/A'\n",
    "\n",
    "# 1. EXPERIMENT CONFIGURATION TABLE\n",
    "config_data = {\n",
    "    'Experiment_Info': {\n",
    "        'Experiment_ID': experiment_id,\n",
    "        'Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'Platform': 'Google Colab',\n",
    "        'GPU': gpu_name,\n",
    "        'GPU_Memory_GB': gpu_memory\n",
    "    },\n",
    "    'Dataset_Config': {\n",
    "        'Dataset_Name': 'medalpaca/medical_meadow_medical_flashcards',\n",
    "        'Total_Samples': 5000,\n",
    "        'Train_Samples': len(train_dataset),\n",
    "        'Val_Samples': len(val_dataset),\n",
    "        'Test_Samples': len(test_dataset),\n",
    "        'Train_Split': '85%',\n",
    "        'Val_Split': '10%',\n",
    "        'Test_Split': '5%'\n",
    "    },\n",
    "    'Model_Config': {\n",
    "        'Base_Model': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "        'Model_Size': '1.1B parameters',\n",
    "        'Fine_tuning_Method': 'LoRA (Low-Rank Adaptation)',\n",
    "        'Quantization': '8-bit',\n",
    "        'LoRA_Rank_r': peft_config.r,\n",
    "        'LoRA_Alpha': peft_config.lora_alpha,\n",
    "        'LoRA_Dropout': peft_config.lora_dropout,\n",
    "        'Target_Modules': ', '.join(peft_config.target_modules)\n",
    "    },\n",
    "    'Training_Config': {\n",
    "        'Num_Epochs': training_args.num_train_epochs,\n",
    "        'Batch_Size': training_args.per_device_train_batch_size,\n",
    "        'Gradient_Accumulation': training_args.gradient_accumulation_steps,\n",
    "        'Effective_Batch_Size': training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,\n",
    "        'Learning_Rate': training_args.learning_rate,\n",
    "        'Warmup_Steps': training_args.warmup_steps,\n",
    "        'LR_Scheduler': training_args.lr_scheduler_type,\n",
    "        'Max_Seq_Length': 512,\n",
    "        'Optimizer': 'AdamW (paged_adamw_8bit)',\n",
    "        'FP16': training_args.fp16\n",
    "    },\n",
    "    'Training_Results': {\n",
    "        'Initial_Loss': f'{initial_loss:.6f}' if 'initial_loss' in locals() else 'N/A',\n",
    "        'Final_Loss': f'{final_loss:.6f}' if 'final_loss' in locals() else 'N/A',\n",
    "        'Loss_Improvement': f'{loss_improvement:.2f}%' if 'loss_improvement' in locals() else 'N/A',\n",
    "        'Total_Training_Steps': train_steps[-1] if 'train_steps' in locals() and train_steps else 'N/A',\n",
    "        'Training_Time': 'See Colab execution time'\n",
    "    },\n",
    "    'Evaluation_Results': {\n",
    "        'Avg_Base_Score': f'{avg_base:.4f}' if 'avg_base' in locals() else 'N/A',\n",
    "        'Avg_Finetuned_Score': f'{avg_ft:.4f}' if 'avg_ft' in locals() else 'N/A',\n",
    "        'Overall_Improvement': f'{avg_improvement:.2f}%' if 'avg_improvement' in locals() else 'N/A',\n",
    "        'Best_Metric': metrics_names[improvements.index(max(improvements))].upper() if 'improvements' in locals() else 'N/A',\n",
    "        'Best_Improvement': f'{max(improvements):.2f}%' if 'improvements' in locals() else 'N/A'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "config_rows = []\n",
    "for category, items in config_data.items():\n",
    "    config_rows.append(['', '', ''])  # Empty row as separator\n",
    "    config_rows.append([f'=== {category.replace(\"_\", \" \").upper()} ===', '', ''])\n",
    "    for key, value in items.items():\n",
    "        config_rows.append([category, key.replace('_', ' '), str(value)])\n",
    "\n",
    "config_df = pd.DataFrame(config_rows, columns=['Category', 'Parameter', 'Value'])\n",
    "\n",
    "# Save configuration table\n",
    "config_path = 'results/experiments/experiment_configuration.csv'\n",
    "config_df.to_csv(config_path, index=False)\n",
    "print(f'✓ Saved: {config_path}')\n",
    "\n",
    "# 2. CREATE DETAILED EXPERIMENT LOG\n",
    "log_content = f\"\"\"\n",
    "{'='*80}\n",
    "MEDICAL LLM FINE-TUNING - COMPLETE EXPERIMENT LOG\n",
    "{'='*80}\n",
    "\n",
    "EXPERIMENT IDENTIFICATION\n",
    "{'='*80}\n",
    "Experiment ID:      {experiment_id}\n",
    "Date & Time:        {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Platform:           Google Colab\n",
    "GPU:                {gpu_name}\n",
    "GPU Memory:         {gpu_memory} GB\n",
    "\n",
    "{'='*80}\n",
    "DATASET CONFIGURATION\n",
    "{'='*80}\n",
    "Dataset:            medalpaca/medical_meadow_medical_flashcards\n",
    "Total Samples:      5,000 (subset from 33,955)\n",
    "Training:           {len(train_dataset):,} samples (85%)\n",
    "Validation:         {len(val_dataset):,} samples (10%)\n",
    "Test:               {len(test_dataset):,} samples (5%)\n",
    "\n",
    "Data Format:        Instruction-Response pairs\n",
    "Domain:             Medical Q&A\n",
    "Preprocessing:      Prompt formatting, train/val/test split\n",
    "\n",
    "{'='*80}\n",
    "MODEL CONFIGURATION\n",
    "{'='*80}\n",
    "Base Model:         TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "Model Size:         1.1 billion parameters\n",
    "Architecture:       Causal Language Model (Chat-optimized)\n",
    "\n",
    "Fine-tuning Method: LoRA (Low-Rank Adaptation)\n",
    "- LoRA Rank (r):    {peft_config.r}\n",
    "- LoRA Alpha:       {peft_config.lora_alpha}\n",
    "- LoRA Dropout:     {peft_config.lora_dropout}\n",
    "- Target Modules:   {', '.join(peft_config.target_modules)}\n",
    "- Trainable Params: ~0.5% of total\n",
    "\n",
    "Quantization:       8-bit (Memory optimization)\n",
    "- Load in 8-bit:    True\n",
    "- Double Quant:     True\n",
    "- Memory Savings:   ~50% reduction\n",
    "\n",
    "{'='*80}\n",
    "TRAINING CONFIGURATION\n",
    "{'='*80}\n",
    "Number of Epochs:           {training_args.num_train_epochs}\n",
    "Batch Size:                 {training_args.per_device_train_batch_size}\n",
    "Gradient Accumulation:      {training_args.gradient_accumulation_steps}\n",
    "Effective Batch Size:       {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\n",
    "\n",
    "Learning Rate:              {training_args.learning_rate}\n",
    "LR Scheduler:               {training_args.lr_scheduler_type}\n",
    "Warmup Steps:               {training_args.warmup_steps}\n",
    "\n",
    "Optimizer:                  paged_adamw_8bit\n",
    "Max Sequence Length:        512 tokens\n",
    "FP16 Training:              {training_args.fp16}\n",
    "\n",
    "Logging Steps:              {training_args.logging_steps}\n",
    "Save Steps:                 {training_args.save_steps}\n",
    "\n",
    "{'='*80}\n",
    "TRAINING RESULTS\n",
    "{'='*80}\n",
    "Initial Loss:               {f'{initial_loss:.6f}' if 'initial_loss' in locals() else 'N/A'}\n",
    "Final Loss:                 {f'{final_loss:.6f}' if 'final_loss' in locals() else 'N/A'}\n",
    "Minimum Loss:               {f'{min_loss:.6f}' if 'min_loss' in locals() else 'N/A'}\n",
    "Average Loss:               {f'{avg_loss:.6f}' if 'avg_loss' in locals() else 'N/A'}\n",
    "\n",
    "Loss Improvement:           {f'{loss_improvement:.2f}%' if 'loss_improvement' in locals() else 'N/A'} {'(Reduction)' if 'loss_improvement' in locals() else ''}\n",
    "Total Steps:                {f'{train_steps[-1]:,}' if 'train_steps' in locals() and train_steps else 'N/A'}\n",
    "\n",
    "Training completed successfully ✓\n",
    "\n",
    "{'='*80}\n",
    "EVALUATION RESULTS (Base vs Fine-tuned)\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "# Add metric-by-metric comparison\n",
    "if 'base_metrics' in locals() and 'finetuned_metrics' in locals():\n",
    "    log_content += \"\\nMetric Comparison:\\n\"\n",
    "    log_content += f\"{'Metric':<20} {'Base':>12} {'Fine-tuned':>12} {'Improvement':>12}\\n\"\n",
    "    log_content += f\"{'-'*60}\\n\"\n",
    "    for metric in metrics_names:\n",
    "        base_val = base_metrics[metric]\n",
    "        ft_val = finetuned_metrics[metric]\n",
    "        imp = ((ft_val - base_val) / base_val * 100) if base_val > 0 else 0\n",
    "        log_content += f\"{metric.upper():<20} {base_val:>12.4f} {ft_val:>12.4f} {imp:>11.2f}%\\n\"\n",
    "    \n",
    "    log_content += f\"{'-'*60}\\n\"\n",
    "    log_content += f\"{'AVERAGE':<20} {avg_base:>12.4f} {avg_ft:>12.4f} {avg_improvement:>11.2f}%\\n\"\n",
    "    log_content += f\"\\n✓ Overall Performance Improvement: {avg_improvement:+.2f}%\\n\"\n",
    "    log_content += f\"✓ Best Performing Metric: {metrics_names[improvements.index(max(improvements))].upper()} ({max(improvements):.2f}%)\\n\"\n",
    "\n",
    "log_content += f\"\"\"\n",
    "{'='*80}\n",
    "GENERATED FILES & OUTPUTS\n",
    "{'='*80}\n",
    "\n",
    "Visualizations:\n",
    "   results/visualizations/data_exploration/comprehensive_data_analysis.png\n",
    "   results/visualizations/preprocessing/data_split_analysis.png\n",
    "   results/visualizations/training/training_progress.png\n",
    "   results/visualizations/evaluation/comprehensive_evaluation.png\n",
    "   results/visualizations/evaluation/example_predictions.png\n",
    "\n",
    "Metrics & Tables:\n",
    "   results/metrics/data_exploration_statistics.csv\n",
    "   results/metrics/preprocessing_summary.csv\n",
    "   results/metrics/training_metrics.csv\n",
    "   results/metrics/evaluation_results.csv\n",
    "   results/metrics/example_predictions.csv\n",
    "\n",
    "Models:\n",
    "   results/models/finetuned_medical_llm/ (Fine-tuned model with LoRA)\n",
    "   results/models/base_model_metrics.json\n",
    "   results/models/finetuned_model_metrics.json\n",
    "\n",
    "Experiment Tracking:\n",
    "   results/experiments/experiment_{experiment_id}.txt (This log)\n",
    "   results/experiments/experiment_configuration.csv\n",
    "\n",
    "{'='*80}\n",
    "CONCLUSIONS & RECOMMENDATIONS\n",
    "{'='*80}\n",
    "\n",
    "1. Training Success:\n",
    "   {' Model successfully fine-tuned with significant loss reduction' if 'loss_improvement' in locals() and loss_improvement > 0 else '- Check training metrics'}\n",
    "\n",
    "2. Evaluation Results:\n",
    "   {' Fine-tuned model outperforms base model across metrics' if 'avg_improvement' in locals() and avg_improvement > 0 else '- Check evaluation results'}\n",
    "\n",
    "3. Next Steps:\n",
    "   - Deploy model for inference testing\n",
    "   - Integrate with FastAPI backend\n",
    "   - Create React UI for user interaction\n",
    "   - Prepare demo video for assignment submission\n",
    "   - Document results in final report\n",
    "\n",
    "4. Model Ready for:\n",
    "   - Real-time medical Q&A\n",
    "   - Integration into medical assistant application\n",
    "   - Further fine-tuning with domain-specific data\n",
    "   - Deployment to production environment\n",
    "\n",
    "{'='*80}\n",
    "EXPERIMENT COMPLETED SUCCESSFULLY ✓\n",
    "{'='*80}\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "# Save detailed log\n",
    "log_path = f'results/experiments/experiment_{experiment_id}.txt'\n",
    "with open(log_path, 'w') as f:\n",
    "    f.write(log_content)\n",
    "print(f'✓ Saved: {log_path}')\n",
    "\n",
    "# Display summary\n",
    "print(log_content)\n",
    "\n",
    "# 3. CREATE EXPERIMENT SUMMARY VISUALIZATION\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle(f'Experiment Summary - {experiment_id}', fontsize=18, fontweight='bold')\n",
    "\n",
    "# 1. Key Metrics Summary\n",
    "ax1 = axes[0, 0]\n",
    "ax1.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "EXPERIMENT SUMMARY\n",
    "{'='*50}\n",
    "\n",
    "ID: {experiment_id}\n",
    "Date: {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "MODEL: TinyLlama-1.1B + LoRA\n",
    "METHOD: 8-bit Fine-tuning\n",
    "\n",
    "DATASET: Medical Flashcards\n",
    "- Training: {len(train_dataset):,} samples\n",
    "- Validation: {len(val_dataset):,} samples  \n",
    "- Test: {len(test_dataset):,} samples\n",
    "\n",
    "TRAINING:\n",
    "- Epochs: {training_args.num_train_epochs}\n",
    "- Loss Improvement: {loss_improvement:.2f}% {'↓' if 'loss_improvement' in locals() else ''}\n",
    "\n",
    "EVALUATION:\n",
    "- Avg Improvement: {avg_improvement:.2f}% {'↑' if 'avg_improvement' in locals() and avg_improvement > 0 else ''}\n",
    "- Best Metric: {metrics_names[improvements.index(max(improvements))].upper() if 'improvements' in locals() else 'N/A'}\n",
    "\n",
    "STATUS: ✓ COMPLETED\n",
    "\"\"\"\n",
    "ax1.text(0.1, 0.5, summary_text, fontsize=12, family='monospace',\n",
    "         verticalalignment='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "\n",
    "# 2. Files Generated Count\n",
    "ax2 = axes[0, 1]\n",
    "file_categories = ['Visualizations', 'Metrics', 'Models', 'Experiments']\n",
    "file_counts = [5, 5, 3, 2]  # Counts of files in each category\n",
    "colors_files = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "\n",
    "bars = ax2.bar(file_categories, file_counts, color=colors_files, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Number of Files', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Generated Output Files', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(0, max(file_counts) + 2)\n",
    "\n",
    "for bar, count in zip(bars, file_counts):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{count}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. Training Configuration\n",
    "ax3 = axes[1, 0]\n",
    "ax3.axis('tight')\n",
    "ax3.axis('off')\n",
    "\n",
    "config_table_data = [\n",
    "    ['Configuration', 'Value'],\n",
    "    ['LoRA Rank', str(peft_config.r)],\n",
    "    ['LoRA Alpha', str(peft_config.lora_alpha)],\n",
    "    ['Learning Rate', f'{training_args.learning_rate}'],\n",
    "    ['Batch Size', f'{training_args.per_device_train_batch_size}'],\n",
    "    ['Epochs', str(training_args.num_train_epochs)],\n",
    "    ['Quantization', '8-bit'],\n",
    "]\n",
    "\n",
    "table = ax3.table(cellText=config_table_data, cellLoc='left', loc='center',\n",
    "                  colWidths=[0.5, 0.5])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "table[(0, 0)].set_facecolor('#40466e')\n",
    "table[(0, 1)].set_facecolor('#40466e')\n",
    "table[(0, 0)].set_text_props(weight='bold', color='white')\n",
    "table[(0, 1)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "for i in range(1, len(config_table_data)):\n",
    "    table[(i, 0)].set_facecolor('#f0f0f0')\n",
    "    table[(i, 0)].set_text_props(weight='bold')\n",
    "    table[(i, 1)].set_facecolor('white')\n",
    "\n",
    "ax3.set_title('Hyperparameters', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# 4. Quick Results\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('tight')\n",
    "ax4.axis('off')\n",
    "\n",
    "results_table_data = [\n",
    "    ['Metric', 'Result'],\n",
    "    ['Training Loss ↓', f'{loss_improvement:.1f}%' if 'loss_improvement' in locals() else 'N/A'],\n",
    "    ['Eval Improvement ↑', f'{avg_improvement:.1f}%' if 'avg_improvement' in locals() else 'N/A'],\n",
    "    ['Best Metric', f'{metrics_names[improvements.index(max(improvements))].upper()}' if 'improvements' in locals() else 'N/A'],\n",
    "    ['Status', '✓ SUCCESS'],\n",
    "]\n",
    "\n",
    "table2 = ax4.table(cellText=results_table_data, cellLoc='center', loc='center',\n",
    "                   colWidths=[0.5, 0.5])\n",
    "table2.auto_set_font_size(False)\n",
    "table2.set_fontsize(11)\n",
    "table2.scale(1, 2.5)\n",
    "\n",
    "table2[(0, 0)].set_facecolor('#40466e')\n",
    "table2[(0, 1)].set_facecolor('#40466e')\n",
    "table2[(0, 0)].set_text_props(weight='bold', color='white')\n",
    "table2[(0, 1)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "for i in range(1, len(results_table_data)):\n",
    "    table2[(i, 0)].set_facecolor('#f0f0f0')\n",
    "    table2[(i, 0)].set_text_props(weight='bold')\n",
    "    if i == len(results_table_data) - 1:\n",
    "        table2[(i, 1)].set_facecolor('lightgreen')\n",
    "        table2[(i, 1)].set_text_props(weight='bold')\n",
    "    else:\n",
    "        table2[(i, 1)].set_facecolor('white')\n",
    "\n",
    "ax4.set_title('Quick Results', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save experiment summary visualization\n",
    "summary_viz_path = f'results/experiments/experiment_{experiment_id}_summary.png'\n",
    "plt.savefig(summary_viz_path, dpi=300, bbox_inches='tight')\n",
    "print(f' Saved: {summary_viz_path}')\n",
    "plt.show()\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print(' COMPREHENSIVE EXPERIMENT TRACKING COMPLETE')\n",
    "print('='*80)\n",
    "print(f'\\n All results saved to: results/')\n",
    "print(f' Total files generated: 15+')\n",
    "print(f' Experiment ID: {experiment_id}')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1fbea",
   "metadata": {},
   "source": [
    "## Experiment Tracking - Complete Configuration & Results\n",
    "\n",
    "**What this does:** Creates a comprehensive experiment tracking report including:\n",
    "- All hyperparameters and configurations\n",
    "- Complete training metrics\n",
    "- Evaluation results\n",
    "- Model comparison summary\n",
    "- Resource usage\n",
    "- File locations\n",
    "\n",
    "**Files saved:**\n",
    "- Complete experiment configuration table\n",
    "- Full experiment log with all settings\n",
    "- Summary report for documentation\n",
    "\n",
    "**Why it's important:** Complete tracking enables reproducibility and comparison with future experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
