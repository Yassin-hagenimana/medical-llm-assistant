Category,Parameter,Value
,,
=== EXPERIMENT INFO ===,,
Experiment_Info,Experiment ID,20260217_192824
Experiment_Info,Date,2026-02-17 20:39:18
Experiment_Info,Platform,Google Colab
Experiment_Info,GPU,Tesla T4
Experiment_Info,GPU Memory GB,15.64
,,
=== DATASET CONFIG ===,,
Dataset_Config,Dataset Name,medalpaca/medical_meadow_medical_flashcards
Dataset_Config,Total Samples,5000
Dataset_Config,Train Samples,2550
Dataset_Config,Val Samples,301
Dataset_Config,Test Samples,149
Dataset_Config,Train Split,85%
Dataset_Config,Val Split,10%
Dataset_Config,Test Split,5%
,,
=== MODEL CONFIG ===,,
Model_Config,Base Model,TinyLlama/TinyLlama-1.1B-Chat-v1.0
Model_Config,Model Size,1.1B parameters
Model_Config,Fine tuning Method,LoRA (Low-Rank Adaptation)
Model_Config,Quantization,8-bit
Model_Config,LoRA Rank r,16
Model_Config,LoRA Alpha,32
Model_Config,LoRA Dropout,0.05
Model_Config,Target Modules,"o_proj, v_proj, k_proj, q_proj"
,,
=== TRAINING CONFIG ===,,
Training_Config,Num Epochs,3
Training_Config,Batch Size,2
Training_Config,Gradient Accumulation,4
Training_Config,Effective Batch Size,8
Training_Config,Learning Rate,0.0002
Training_Config,Warmup Steps,100
Training_Config,LR Scheduler,SchedulerType.COSINE
Training_Config,Max Seq Length,512
Training_Config,Optimizer,AdamW (paged_adamw_8bit)
Training_Config,FP16,True
,,
=== TRAINING RESULTS ===,,
Training_Results,Initial Loss,14.796332
Training_Results,Final Loss,0.223441
Training_Results,Loss Improvement,98.49%
Training_Results,Total Training Steps,950
Training_Results,Training Time,See Colab execution time
,,
=== EVALUATION RESULTS ===,,
Evaluation_Results,Avg Base Score,0.1796
Evaluation_Results,Avg Finetuned Score,0.2340
Evaluation_Results,Overall Improvement,30.31%
Evaluation_Results,Best Metric,BLEU_4
Evaluation_Results,Best Improvement,52.71%
